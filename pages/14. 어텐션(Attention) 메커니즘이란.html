<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Attention 심층 분석</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@400;500;700&display=swap" rel="stylesheet">
    
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <style>
        body {
            font-family: 'Noto Sans KR', sans-serif;
            background-color: #f7f8fc;
            color: #495057;
            margin: 0;
            padding: 0;
            line-height: 1.7;
            -webkit-font-smoothing: antialiased;
        }

        .container {
            max-width: 900px;
            margin: 40px auto;
            padding: 0 20px;
        }

        h1 {
            font-size: 2.5rem;
            font-weight: 700;
            color: #212529;
            text-align: center;
            margin-bottom: 12px;
        }
        
        h1.section-title {
            font-size: 2rem;
            margin-bottom: 16px;
        }

        p.subtitle {
            font-size: 1.15rem;
            color: #6c757d;
            text-align: center;
            margin-top: 0;
            margin-bottom: 40px;
        }
        
        p.section-intro {
            font-size: 1.1rem;
            color: #6c757d;
            text-align: center;
            max-width: 700px;
            margin: 0 auto 30px auto;
        }

        h2 {
            font-size: 1.75rem;
            font-weight: 700;
            color: #212529;
            text-align: center;
            margin: 60px 0 30px 0;
        }

        h3 {
            font-size: 1.4rem;
            font-weight: 700;
            color: #212529;
            margin-top: 0;
            margin-bottom: 12px;
        }

        h4 {
            font-size: 1.1rem;
            font-weight: 700;
            color: #343a40;
            margin-top: 0;
            margin-bottom: 12px;
        }

        p {
            margin-top: 0;
            margin-bottom: 1rem;
            color: #495057;
        }
        
        p.secondary {
             color: #6c757d;
             font-size: 1.05rem;
        }

        strong, b {
            font-weight: 700;
            color: #343a40;
        }

        hr {
            border: 0;
            height: 1px;
            background-color: #dee2e6;
            margin: 60px 0;
        }

        .card {
            border: 1px solid #e9ecef;
            border-radius: 12px;
            margin-bottom: 24px;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.04);
            overflow: hidden;
        }

        .conceptual-card {
            background-color: #f8f9fa;
            padding: 24px 30px;
            transition: transform 0.2s ease, box-shadow 0.2s ease;
        }

        .conceptual-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 8px 20px rgba(0, 0, 0, 0.08);
        }

        .sub-card {
            background-color: #ffffff;
            border: 1px solid #e9ecef;
            border-radius: 8px;
            padding: 20px 24px;
            margin-top: 20px;
            box-shadow: 0 2px 6px rgba(0, 0, 0, 0.03);
        }
        
        /* 1. QKV 카드 스타일 */
        .qkv-card {
            background-color: #f1f3f5; /* 연한 어두운색 */
            border-color: #dee2e6;
        }
        
        /* 2, 5. 중첩 카드 스타일 */
        .sub-card .sub-card {
            background-color: #fdfdfd; /* 흰색에 가까운 연한 회색 */
            border-color: #f1f3f5;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.02);
            margin-top: 16px;
        }
        
        .side-by-side-container {
            display: flex;
            gap: 20px;
            margin-top: 20px;
        }

        .side-by-side-container > .sub-card {
            flex: 1;
            margin-top: 0;
        }
        
        pre {
            background-color: #f5f7fa;
            border: 1px solid #e0e6ed;
            border-radius: 6px;
            padding: 16px;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
            font-size: 0.9rem;
            line-height: 1.6;
            overflow-x: auto;
            margin-top: 10px;
            margin-bottom: 1rem;
        }
        
        table {
            width: 100%;
            margin-top: 20px;
            border-collapse: collapse;
            background-color: #ffffff;
            border: 1px solid #e9ecef;
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 2px 6px rgba(0, 0, 0, 0.03);
        }
        th, td {
            border: 1px solid #e9ecef;
            padding: 14px 16px;
            text-align: left;
            vertical-align: top;
        }
        th {
            background-color: #f8f9fa;
            font-weight: 700;
            color: #212529;
        }
        td {
            font-size: 0.95rem;
        }
        tr:not(:last-child) td {
            border-bottom: 1px solid #e9ecef;
        }

        @media (max-width: 768px) {
            .side-by-side-container {
                flex-direction: column;
            }
        }
    </style>
</head>
<body>

    <div class="container">

        <h1>Attention :<br> Seq2seq 모델의 한계부터 Transformer까지</h1>
        <hr>
        <h1 class="section-title">1. Attention의 등장 배경</h1>
        <p class="section-intro">모든 것은 '더 긴 문장을 더 잘 번역하고 싶다'는 단순한 필요에서 시작되었습니다.<br>기존 모델이 가졌던 한계점을 살펴봅니다.</p>
        
        <div class="card conceptual-card">
            <h3>Seq2seq 모델의 한계</h3>
            <p class="secondary">기존의 RNN(Recurrent Neural Network) 기반 <strong>Seq2seq (Sequence-to-Sequence)</strong> 모델은 
            <br><b>번역</b>과 같은 작업에서 획기적이었지만, 명확한 구조적 한계를 가지고 있었습니다.</p>
            <p class="secondary">이 구조는 <strong>'암기식 시험'</strong>과 같았습니다. 인코더(Encoder)가 입력 문장("나는 학생이다") 전체를 읽고, 
            <br>그 의미를 <strong>하나의 고정된 크기 벡터</strong>, 즉 <strong>컨텍스트 벡터(Context Vector)</strong>로 압축해야 했습니다. 
            <br>디코더(Decoder)는 오직 이 '하나의 요약 노트'에만 의존해서 "I am a student"라는 출력 문장을 생성해야 했죠.</p>
            <p class="secondary">이러한 구조는 다음과 같은 두 가지 큰 문제를 야기했습니다.</p>
            
            <div class="sub-card">
                <h3>1. 정보 병목 현상 (Information Bottleneck)</h3>
                <p>입력 시퀀스의 길이가 길어질수록 (문장이 길어질수록), 모든 정보를 하나의 <b>고정된 크기 벡터</b>에 욱여넣는 과정에서 
                <br><strong>정보 손실</strong>이 필연적으로 발생했습니다. 특히 문장 앞부분의 정보가 뒤로 갈수록 희석되는 문제가 있었습니다.</p>
            </div>
            
            <div class="sub-card">
                <h3>2. 고정된 표현의 한계</h3>
                <p>디코더는 단어를 생성하는 매 시점(타임스텝)마다 <strong>동일한 컨텍스트 벡터</strong>를 참조해야 했습니다. 
                <br>"I"를 만들 때, "am"을 만들 때, "student"를 만들 때 모두 같은 요약 노트를 봐야 했죠. 이는 각 시점마다 필요한 정보가 다름에도 불구하고, 문맥에 따른 동적인 정보 활용이 불가능함을 의미했습니다.</p>
            </div>
        </div>
        <hr>
        <h1 class="section-title">2. Attention의 핵심 개념</h1>
        <p class="section-intro">'암기식 시험'을 '오픈북 시험'으로 바꾼 혁신적인 아이디어, 어텐션의 핵심 원리를 알아봅니다.</p>
        
        <div class="card conceptual-card">
            <h3>'오픈북 시험'으로의 전환</h3>
            <p class="secondary"><strong>어텐션(Attention)</strong> 메커니즘은 위와 같은 한계를 극복하기 위해 제안되었습니다. 
            <br>핵심 아이디어는 이 방식을 <strong>'오픈북 시험'</strong>으로 바꾼 혁신입니다.</p>
            <p class="secondary">디코더는 더 이상 하나의 요약 노트에만 의존하지 않습니다. 
            <br>대신, 출력 단어를 예측하는 <strong>매 시점마다, 인코더의 전체 입력 시퀀스('원본 참고서')를 다시 한번 참고</strong>하되, 
            <br>현재 예측에 <strong>가장 관련이 높은 특정 부분에 '집중(Attention)'</strong>하여 정보를 활용합니다.</p>
            <p class="secondary">이를 통해 고정된 컨텍스트 벡터가 아닌, 매번 새롭게 계산되는 <strong>동적 컨텍스트 벡터(Dynamic Context Vector)</strong>를 생성하여 정보 병목 현상을 해결합니다. 이 과정의 핵심이 바로 <strong>'가중합(Weighted Sum)'</strong>입니다.</p>
        </div>
        <hr>
        <h1 class="section-title">3. Attention의 핵심 구성 요소</h1>
        <p class="section-intro">어텐션을 작동하게 하는 세 가지 핵심 부품,<br> Query, Key, Value에 대해 '검색 엔진' 비유로 쉽게 이해합니다.</p>
        
        <div class="card conceptual-card">
            <h3>Query, Key, Value (Q, K, V)</h3>
            <p class="secondary">어텐션 메커니즘은 세 가지 주요 구성 요소인 <strong>Query, Key, Value</strong> 벡터의 상호작용으로 동작합니다. 
            <br>'검색 엔진'으로 비유하면 이해하기 쉽습니다.</p>
            
            <div class="side-by-side-container">
                <div class="sub-card qkv-card">
                    <h4>Query (Q) :
                    <br>"내가 지금 궁금한 것"<br>(검색어)</h4>
                    <p>: 현재 <b>디코더의 상태</b>를 나타내는 벡터입니다. 
                    <br>출력 시퀀스의 특정 위치에서 "어떤 정보가 필요한가?"를 질의하는 역할을 합니다. 
                    <br><strong>(디코더의 은닉 상태 벡터 \(s_t\)</strong> 사용)
                    </p>
                </div>
                <div class="sub-card qkv-card">
                    <h4>Key (K) :
                    <br>"정보를 찾기 위한 색인"<br>(라벨)</h4>
                    <p>인코더의 <b>모든 시점의 은닉 상태 벡터들</b>입니다. 
                    <br>Query와 연관성을 계산하기 위한 '색인' 역할을 하며, 
                    <br>각 Key는 특정 입력 단어의 정보를 담고 있습니다.
                    <br><strong>(인코더의 모든 은닉 상태 벡터들 \(h_1, h_2, ..., h_n\)</strong> 사용)
                    </p>
                    
                </div>
                <div class="sub-card qkv-card">
                    <h4>Value (V) :
                    <br>"실제 정보 내용" (내용물)</h4>
                    <p>Key와 1:1로 매핑되는 벡터로, 
                    <br>Key와 마찬가지로 <b>인코더의 모든 시점의 은닉 상태 벡터들 \(h_1, h_2, ..., h_n\)</b>입니다. Key를 통해 연관성이 결정되면, 
                    <br><b>실제 디코더에 전달되는 '실질적인 내용'입니다.</b><br>
                    <strong>(인코더의 모든 은닉 상태 벡터들 \(h_1, h_2, ..., h_n\)</strong> 사용)
                    </p>
                </div>
            </div>

            <div class="sub-card">
                <h4>Q. Key와 Value 모두 "인코더의 동일한 은닉벡터 상태"를 사용하는데, 구분하는 이유가 뭘까?</h4>
                <p>많은 초기 어텐션 모델(Cross-Attention)에서는 Key와 Value가 <b>인코더의 동일한 은닉 상태 벡터를 사용합니다.</b> 
                <br>하지만 개념적으로 이 둘을 분리함으로써 유연성을 확보할 수 있습니다. 
                <br><strong>Key</strong>는 <b>Query와의</b> <strong>'유사도 계산'</strong>이라는 특정 목적에, 
                <br><strong>Value</strong>는 <strong>'정보의 가중합'</strong>이라는 다른 목적에 사용될 수 있도록 설계된 것입니다. 
                <br>이는 이후 셀프 어텐션과 Multi-Head Attention에서 더욱 중요해집니다.</p>
            </div>
        </div>
        <hr>
        <div class="Attention 전체 동작 과정">
            <h1 class="section-title">4. Attention 전체 동작 과정</h1>
            <p class="section-intro">Attention 메커니즘의 전체 동작 과정을 시각적으로 나타낸 애니메이션입니다.</p>
            <img src="../assets/attention_animation.gif" alt="Attention Mechanism Animation" style="width:100%; border-radius:12px; box-shadow: 0 4px 12px rgba(0, 0, 0, 0.04);">
            <img src="../assets/attention.png" alt="Attention Mechanism Animation" style="width:100%; border-radius:12px; box-shadow: 0 4px 12px rgba(0, 0, 0, 0.04);">
        </div>
        <hr>
        <h1 class="section-title">5. Attention의 동작과정 설명</h1>
        <center><p class="secondary">"Thank you"를 "고마워"로 번역하는 과정을 통해 어텐션의 연산 흐름을 단계별로 살펴보겠습니다.</p></center>
        <br>
        <div class="card conceptual-card">
            <div class="sub-card">
                <h3>1단계 : 인코더 - Key와 Value 준비 (참고서 준비)</h3>
                
                <div class="sub-card">
                    <h4>1. 입력 처리</h4>
                    <p>인코더는 "Thank", "you", &lt;EOS&gt;(문장 끝)를 순서대로 읽습니다.</p>
                </div>
                
                <div class="sub-card">
                    <h4>2. 은닉 상태 기록</h4>
                    <p>기존 Seq2seq와 달리, 마지막 은닉 상태만 사용하는 것이 아니라 <strong>모든 시점의 은닉 상태 벡터들</strong>을 <B>저장</B>합니다.<br>
                    \(h_{thank}\) = "Thank"까지 읽었을 때의 정보<br>
                    \(h_{you}\) = "Thank you"까지 읽었을 때의 정보<br>
                    \(h_{eos}\) = 문장 전체를 읽었을 때의 정보</p>
                </div>

                <div class="sub-card">
                    <h4>3. K, V 생성</h4>
                    <p>이 은닉 상태 벡터의 집합이 <strong>Keys (K)</strong>이자 <strong>Values (V)</strong>가 됩니다.</p>
                    <pre>Keys = [\(h_{thank}\), \(h_{you}\), \(h_{eos}\)]
Values = [\(h_{thank}\), \(h_{you}\), \(h_{eos}\)]</pre>
                </div>
            </div>

            <div class="sub-card">
                <h3>2단계 : 디코더 (1번째 스텝) - "고마워" 생성</h3>
                
                <div class="sub-card">
                    <h4>1. Query 생성 : 디코더의 현재 은닉 상태</h4>
                    <p>첫 번째 단어를 예측하기 위한 <b>디코더의 현재 은닉 상태</b> \(s_0\)가 <strong>Query (Q)</strong>가 됩니다. 
                    <br>(즉, "첫 단어 만들 건데, 뭘 참고해야 해?"라는 질문)</p>
                </div>
                
                <div class="sub-card">
                    <h4>2. 어텐션 스코어(Attention Score) 계산</h4>
                    <p>Query \(s_0\)를 모든 Key 벡터들과 비교하여 <b>유사도 점수(관련도)</b>를 계산합니다. 
                    <br>(점수 계산 함수는 <b>내적(Dot-Product)</b> 등 다양한 방식이 사용됩니다.)</p>
                    <pre>score_thank = score(\(s_0\), \(h_{thank}\))
score_you = score(\(s_0\), \(h_{you}\))
score_eos = score(\(s_0\), \(h_{eos}\))</pre>
                    <p>(이때 "고마워"는 "Thank"와 "you" 모두와 관련이 깊으므로, <code>score_thank</code>와 <code>score_you</code>가 높게 나올 것입니다.)</p>
                </div>

                <div class="sub-card">
                    <h4>3. 어텐션 가중치(Attention Weights) 계산</h4>
                    <p>계산된 스코어들에 <strong>소프트맥스(Softmax)</strong> 함수를 적용하여 합이 1인 확률 분포, 
                    <br>즉 어텐션 가중치(\(\alpha\))를 얻습니다. 이 가중치는 각 입력 단어에 얼마나 '집중'할지를 나타내는 <strong>비율</strong>입니다.</p>
                    <pre>예시: weights(\(\alpha_0\)) = [0.5, 0.5, 0.0]</pre>
                    <p>(이는 "Thank"와 "you"의 정보를 각각 50%씩 반영하고 &lt;EOS&gt;는 무시하겠다는 의미입니다.)</p>
                </div>

                <div class="sub-card">
                    <h4>4. 컨텍스트 벡터(Context Vector) 생성</h4>
                    <p>계산된 어텐션 가중치를 각 <strong>Value</strong> 벡터에 곱한 후 모두 더합니다 (<strong>가중합, Weighted Sum</strong>). 
                    <br>이를 통해 현재 타임스텝만을 위한 <strong>맞춤형(동적) 컨텍스트 벡터 \(c_0\)</strong>가 생성됩니다.</p>
                    <pre>\(c_0\) = (0.5 \(\times\) \(V_{thank}\)) + (0.5 \(\times\) \(V_{you}\)) + (0.0 \(\times\) \(V_{eos}\))</pre>
                </div>

                <div class="sub-card">
                    <h4>5. 최종 단어 생성</h4>
                    <p>디코더는 이렇게 생성된 <strong>컨텍스트 벡터 \(c_0\)</strong>와 자신의 <strong>은닉 상태 \(s_0\)</strong>를 결합(Concatenate)하여 
                    <br>최종적으로 "고마워"라는 단어를 예측합니다.</p>
                </div>
            </div>

            <div class="sub-card">
                <h3>3단계 : 디코더 (2번째 스텝) - &lt;EOS&gt; 생성</h3>
                <p>1. 이제 디코더는 방금 생성한 "고마워"를 다음 입력으로 받고, 자신의 은닉 상태를 \(s_1\)로 업데이트합니다.
                <br>2. 이 <strong>새로운 은닉 상태 \(s_1\)</strong>이 <strong>새로운 Query(Q)</strong>가 되어 위 2~5번 과정을 반복합니다.
                <br>3. 이제 문장이 끝났음을 알아채야 하므로, 인코더의 마지막 기록인 \(h_{eos}\)와의 관련도 점수가 가장 높게 계산될 것입니다.<br>(예: <code>weights</code>(\(\alpha_1\)) = <code>[0.1, 0.1, 0.8]</code>)
                <br>4. 이 가중치로 새로운 컨텍스트 벡터 \(c_1\)이 만들어지고,<br>디코더는 이를 바탕으로 문장의 끝을 의미하는 &lt;EOS&gt; 토큰을 출력하며 번역을 마칩니다.</p>
            </div>
        </div>
        <hr>
        <h1 class="section-title">6. 손실(Loss)과 학습 기법</h1>
        <p class="section-intro">모델이 어떻게 '정답'을 배우는지, 그리고 더 빠르고 안정적으로 학습하기 위해 사용하는<br>특별한 훈련 기법(교사 강요)은 무엇인지 알아봅니다.</p>
        <div class="card conceptual-card">
            <h3>모델이 '집중'하는 법을 배우는 과정</h3>
            
            <div class="sub-card">
                <h4>5-1. 손실 (Loss)</h4>
                <p><strong>손실(Loss)</strong>은 모델이 내놓은 <b>'예측값'(prediction)</b>과 우리가 알고 있는 <b>'실제 정답'(ground truth)</b> 사이의<br>
                불일치 정도를 정량화한 지표입니다. 학생이 쓴 '오답'과 '정답'의 차이를 '틀린 정도'로 점수화하는 것과 같습니다.</p>
                
                <div class="sub-card">
                    <h4>예시</h4>
                    <p>모델이 "a"를 예측해야 할 상황에서 "am"을 (높은 확률로) 예측했습니다.</p>
                </div>
                
                <div class="sub-card">
                    <h4>계산</h4>
                    <p>이때 모델은 정답인 "a"와 예측값인 "am" 사이의 차이<br>(주로 <strong>크로스 엔트로피 손실(Cross-Entropy Loss)</strong> 사용)를 계산합니다.</p>
                </div>

                <div class="sub-card">
                    <h4>학습</h4>
                    <p>딥러닝 모델은 이 손실 값을 최소화하는 방향(즉, '틀린 정도'를 줄여나가는 방향)으로
                    <br>내부 파라미터(가중치)를 업데이트(역전파, Back-propagation)하며 학습을 진행합니다.</p>
                </div>
            </div>

            <div class="sub-card">
                <h4>5-2. 교사 강요 (Teacher Forcing)</h4>
                <p><strong>교사 강요(Teacher Forcing)</strong>는 Seq2seq 모델의 효율적인 훈련을 위한 기법입니다. 디코더가 다음 단어를 예측할 때, 이전 타임스텝에서 <strong>모델 자신이 예측한 단어</strong>를 입력으로 사용하는 대신, <strong>실제 정답 단어(ground truth)</strong>를 강제로 입력으로 사용합니다.</p>
                
                <div class="sub-card">
                    <h4>Q. 만약 오답을 그대로 사용한다면? (오류 전파)</h4>
                    <p>모델이 "a" 대신 "am"을 예측했을 때, 이 "am"을 다음 입력으로 사용하면 어떻게 될까요?<br>
                    <strong>잘못된 입력 ("am") \(\rightarrow\) 잘못된 다음 예측 (student가 아닌 엉뚱한 단어)</strong><br>
                    첫 단추를 잘못 끼운 것처럼 그 이후의 모든 예측이 연쇄적으로 틀릴 수 있습니다. 
                    <br>(Error Propagation) 이는 학습을 매우 불안정하게 만듭니다.</p>
                </div>

                <div class="sub-card">
                    <h4>A. 교사 강요의 역할</h4>
                    <p>이런 문제를 방지하기 위해, 모델이 "am"이라고 잘못 예측했더라도,<br>다음 단계 입력으로는 <strong>실제 정답인 "a"</strong>를 넣어줍니다.<br>
                    <strong>- 모델 예측 : "am" (오답) \(\rightarrow\) <br>- 다음 입력 : "a" (정답) \(\rightarrow\) 
                    <br>- 올바른 다음 예측 ("student")을 할 확률 증가</strong><br>
                    이처럼 교사 강요는 모델의 실수가 다음 학습에 연쇄적으로 나쁜 영향을 미치는 것을 방지하여,<br>학습 속도를 높이고 훈련 과정을 안정화하는 데 크게 기여합니다.</p>
                </div>
            </div>
        </div>
        <hr>
        <h1 class="section-title">7. Attention의 종류와 발전</h1>
        <p class="section-intro">어텐션은 하나의 개념에 머무르지 않고, 더 빠르고 정교하게 발전했습니다.<br>RNN과 함께 쓰이던 초기 어텐션부터 RNN을 대체한 셀프 어텐션까지의 진화 과정을 살펴봅니다.</p>
        <br>
        <div class="card conceptual-card">
            <h3>어텐션의 진화 과정</h3>
            <p class="secondary">어텐션은 스코어 계산 방식과 적용 대상에 따라 다양하게 발전했습니다.</p>
            
            <div class="sub-card">
                <h4>바다나우 어텐션 (Bahdanau Attention, Additive)</h4>
                <p>스코어 계산에 작은 신경망(Feed-Forward Network)을 사용하는 <strong>덧셈(additive)</strong> 방식입니다. 
                <br>디코더의 <strong>이전 은닉 상태(\(s_{t-1}\))</strong>를 Query로 사용합니다. (정교하지만 계산량이 많음)</p>
            </div>

            <div class="sub-card">
                <h4>루옹 어텐션 (Luong Attention, Multiplicative)</h4>
                <p>스코어 계산에 내적(Dot-product) 등 간단한 <strong>곱셈(multiplicative)</strong> 방식을 사용합니다. 
                <br>디코더의 <strong>현재 은닉 상태(\(s_t\))</strong>를 Query로 사용하여 더 간단하고 효율적입니다.</p>
            </div>
            
            <div class="sub-card">
                <h4>셀프 어텐션 (Self-Attention)</h4>
                <p>위 두 방식은 <strong>'교차 어텐션(Cross-Attention)'</strong>(디코더 \(\leftrightarrow\) 인코더)입니다. 
                <br>반면, <b>셀프 어텐션</b>은 <strong>하나의 시퀀스 내부(입력 시퀀스 \(\leftrightarrow\) 자기 자신)</strong>에서 단어들끼리 
                <br>서로 Query, Key, Value 역할을 수행하여 관계를 파악합니다.</p>
                <p><strong>예시</strong><br>
                "그 동물은 피곤해서 길을 건너지 않았다. 왜냐하면 <strong>그것</strong>은 지쳤기 때문이다."<br>
                이 문장에서 <strong>'그것'</strong>이 '길'이 아니라 <strong>'동물'</strong>을 지칭함을 파악하는 등 문장 내부의 의존 관계(dependency)를 학습합니다.</p>
            </div>

            <table>
                <thead>
                    <tr>
                        <th>구분</th>
                        <th>바다나우/루옹 어텐션 (Cross-Attention)</th>
                        <th>셀프 어텐션 (Self-Attention)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>목적</strong></td>
                        <td>입력-출력 시퀀스 간의 연관성 파악 (e.g., 번역)</td>
                        <td>단일 시퀀스 내 단어 간의 문맥적 관계 파악</td>
                    </tr>
                    <tr>
                        <td><strong>참조 대상</strong></td>
                        <td>디코더 \(\leftrightarrow\) 인코더 (서로 다른 두 시퀀스)</td>
                        <td>입력 시퀀스 \(\leftrightarrow\) 자기 자신 (동일한 하나의 시퀀스)</td>
                    </tr>
                    <tr>
                        <td><strong>Q, K, V 출처</strong></td>
                        <td><strong>Q</strong> : 디코더의 은닉 상태<br><strong>K, V</strong>: 인코더의 모든 은닉 상태</td>
                        <td><strong>Q, K, V</strong><br>: 모두 동일한 입력 시퀀스로부터 생성</td>
                    </tr>
                    <tr>
                        <td><strong>주요 사용 모델</strong></td>
                        <td>RNN 기반 Seq2seq 모델</td>
                        <td><strong>트랜스포머 (Transformer)</strong> 계열 (BERT, GPT 등)</td>
                    </tr>
                </tbody>
            </table>
        </div>
        <hr>
        <h1 class="section-title">7. Transformer 미리보기</h1>
        <p class="section-intro"><b>"Attention Is All You Need"(어텐션만 있으면 돼)</b>라는 외침과 함께 등장한
        <br>어텐션의 최종 진화형이자, 현대 AI 모델의 기반이 된 트랜스포머를 간단히 맛봅니다.</p>
        <br>
        <div class="card conceptual-card">
            <h3>RNN을 대체한 새로운 패러다임</h3>
            <p class="secondary">2017년, <strong>"Attention Is All You Need"</strong>라는 기념비적인 논문에서 
            <br><strong>트랜스포머(Transformer)</strong>가 등장하며 NLP 분야의 패러다임이 완전히 바뀝니다.</p>

            <div class="sub-card">
                <h4>핵심 변화</h4>
                <p>순차적으로 계산해야만 했던 <strong>RNN(LSTM, GRU)의 순차적 연산 구조를 완전히 배제</strong>하고,
                <br>오직 <strong>셀프 어텐션</strong>만으로 <b>인코더와 디코더</b>를 구성했습니다.</p>
            </div>

            <div class="sub-card">
                <h4>장점</h4>
                <p>RNN의 재귀적인(recurrent) 구조가 사라짐으로써, 문장의 모든 단어를 <strong>한 번에 병렬적으로 처리</strong>할 수 있게 되었습니다.
                <br>이는 GPU 활용을 극대화하여 <strong>학습 속도를 비약적으로 향상</strong>시켰습니다.
                <br>또한, 셀프 어텐션을 통해 문장 내의 <strong>장거리 의존 관계(long-range dependency)</strong> 
                <br>(문장의 맨 앞 단어와 맨 뒤 단어의 관계)를 RNN보다 훨씬 효과적으로 포착할 수 있게 되었습니다.</p>
            </div>

            <div class="sub-card">
                <h4>영향</h4>
                <p>트랜스포머의 등장은 현대 자연어 처리(NLP) 분야를 지배하게 되었으며, 
                <br><strong>BERT, GPT</strong> 등 현존하는 대부분의 SOTA(State-of-the-art) 모델의 기반이 되었습니다.</p>
            </div>

            <div class="sub-card">
                <h4>요약하자면</h4>
                <p>바다나우와 루옹 어텐션은 RNN 기반 Seq2seq 모델의 성능을 극대화하기 위한 <strong>'보조 장치'</strong>입니다.<br>
                반면, <strong>셀프 어텐션</strong>은 <strong>RNN 구조 자체를 대체</strong>하며 <B>병렬 처리와 문맥 이해 능력을 한 차원 높인</B> <strong>'새로운 패러다임'</strong>이며, 
                <br>이것이 바로 현대 LLM의 핵심입니다.</p>
            </div>
        </div>
        <br>
        <div class="footer-section">
             <center><h3>출처 및 참고 자료 📖</h3></center>
             <center><a href="https://www.youtube.com/watch?v=cu8ysaaNAh0" target="_blank">1. 신박AI : Seq2seq+Attention 모델을 소개합니다</a><br></center>
             <center><a href="https://www.youtube.com/watch?v=8E6-emm_QVg&list=PL_iJu012NOxd_lWvBM8RfXeB7nPYDwafn&index=7" target="_blank">2. 혁펜하임 : 어텐션 & 셀프-어텐션 가장 직관적인 설명</a><br></center>
             <center><a href="https://modulabs.co.kr/blog/introducing-attention" target="_blank">3. 모두의연구소 : 어텐션 메커니즘 소개</a></center>
         </div>
    </div>

</body>
</html>
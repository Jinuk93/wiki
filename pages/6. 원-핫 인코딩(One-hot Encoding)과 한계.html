<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>텍스트 벡터화 : 원-핫 인코딩에서 워드 임베딩까지</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@400;700;900&display=swap" rel="stylesheet">
    <style>
        /* 1. Core Design Philosophy & Technical Specs */
        :root {
            --color-text-primary: #212529;
            --color-text-secondary: #6c757d;
            --color-border: #dee2e6;
            --color-bg-white: #ffffff;
            --color-bg-subtle: #f8f9fa;
            --color-step-icon-bg: #2c3e50;
            --font-family-base: 'Noto Sans KR', sans-serif;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: var(--font-family-base);
            font-size: 1rem; /* 16px */
            line-height: 1.7;
            background-color: var(--color-bg-white);
            color: var(--color-text-primary);
            margin: 0;
            padding: 4rem 1rem;
            -webkit-font-smoothing: antialiased;
        }

        /* 2. Typography & Hierarchy */
        h1, h2, h3 {
            font-weight: 700;
            line-height: 1.3;
            margin-top: 0;
        }

        h1 {
            font-size: 2.5rem;
            font-weight: 900;
            margin-bottom: 0.5rem;
        }

        h2 {
            font-size: 2rem;
            margin-bottom: 0.75rem;
        }

        h3 {
            font-size: 1.5rem;
            margin-bottom: 1rem;
        }

        p {
            margin-top: 0;
            margin-bottom: 1rem;
        }
        
        strong, b {
           font-weight: 700;
           color: var(--color-text-primary);
        }

        /* 3. Component Library & Structure */
        .container {
            max-width: 800px;
            margin: 0 auto;
        }

        .header, .section-title {
            text-align: center;
            margin-bottom: 3rem;
        }

        .header p, .section-title p {
            font-size: 1.125rem;
            color: var(--color-text-secondary);
            max-width: 600px;
            margin-left: auto;
            margin-right: auto;
            margin-bottom: 0;
        }

        /* Base Card Style */
        .card {
            background-color: var(--color-bg-white);
            border: 1px solid var(--color-border);
            border-radius: 0.75rem;
            padding: 2rem;
            margin-bottom: 1.5rem;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.02);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        /* Card Hover Effect */
        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 20px rgba(0, 0, 0, 0.07);
        }

        /* Component Variations */
        .model-compare-grid {
            display: grid;
            grid-template-columns: 1fr;
            gap: 1.5rem;
        }

        @media (min-width: 768px) {
            .model-compare-grid {
                grid-template-columns: repeat(2, 1fr);
            }
        }
        
        .model-card h3 {
            font-size: 1.25rem;
            border-bottom: 1px solid var(--color-border);
            padding-bottom: 0.75rem;
            margin-bottom: 1rem;
        }
        
        .steps-grid .card {
            padding: 2.5rem;
        }

        .step-card {
            display: flex;
            flex-direction: column;
            gap: 1.5rem;
        }
        
        @media (min-width: 768px) {
            .step-card {
                flex-direction: row;
                align-items: flex-start;
            }
        }
        
        .step-card-content {
            flex: 1;
        }
        
        .step-number {
            flex-shrink: 0;
            width: 50px;
            height: 50px;
            background-color: var(--color-step-icon-bg);
            color: #fff;
            border-radius: 8px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.75rem;
            font-weight: 700;
            text-shadow: 1px 1px 0px rgba(255, 0, 85, 0.5), -1px -1px 0px rgba(0, 229, 255, 0.5);
        }
        
        .step-card h3 {
            font-size: 1.5rem;
            margin-bottom: 0.75rem;
        }
        
        .step-card .sub-section h4 {
            font-size: 1.2rem;
            font-weight: 700;
            color: var(--color-step-icon-bg);
            margin-top: 2rem;
            margin-bottom: 0.5rem;
            border-bottom: 1px solid var(--color-border);
            padding-bottom: 0.5rem;
        }
        .step-card .sub-section:first-child h4 {
             margin-top: 0;
        }


        /* Nested Aside Block */
        aside {
            background-color: var(--color-bg-subtle);
            border-left: 4px solid var(--color-border);
            padding: 1rem 1.5rem;
            margin: 1.5rem 0;
            border-radius: 0 0.25rem 0.25rem 0;
            font-size: 0.95rem;
        }
        
        aside p:last-child, aside ul:last-child {
            margin-bottom: 0;
        }
        
        aside ul {
            padding-left: 1.2rem;
        }
        
        /* Highlighting */
        .highlight-gray {
            background-color: var(--color-bg-subtle);
        }

        /* Standalone Transition Paragraphs */
        .transition-text {
            max-width: 700px;
            margin: 3rem auto;
            font-size: 1.1rem;
            line-height: 1.8;
            color: #495057;
            text-align: left;
        }
        
        /* 4. Interactivity (Scroll Animation) */
        .fade-in-up {
            opacity: 0;
            transform: translateY(30px);
            transition: opacity 0.6s ease-out, transform 0.6s ease-out;
        }

        .fade-in-up.visible {
            opacity: 1;
            transform: translateY(0);
        }
    </style>
</head>
<body>

    <main class="container">
        <header class="header fade-in-up">
            <h1>텍스트 벡터화<br> : 원-핫 인코딩에서 워드 임베딩까지</h1>
        </header>

        <p class="transition-text fade-in-up">
            자연어 처리(NLP)에서 모델을 학습시키기 위해서는, 문자 형태의 텍스트를 기계가 이해할 수 있는 숫자 형태, 즉 <strong>벡터(Vector)</strong>로 변환하는 과정이 필수적입니다. 이를 <strong>벡터화(Vectorization)</strong>라고 하며, 가장 기초적인 방법인 <strong>원-핫 인코딩</strong>부터 현대적인 방식인 <strong>워드 임베딩</strong>까지 알아보겠습니다.
        </p>

        <div class="section-title fade-in-up">
            <h2>1. 원-핫 인코딩 (One-hot Encoding)</h2>
        </div>

        <section class="content-block card fade-in-up">
            <p>원-핫 인코딩은 각 단어(토큰)를 어휘 집합(Vocabulary)의 크기와 동일한 차원을 갖는 <strong>희소 벡터(Sparse Vector)</strong>로 표현하는 기법입니다. 벡터에서 <strong>해당 단어의 고유한 위치만 1로 표시하고 나머지는 모두 0</strong>으로 채웁니다.</p>

            <h3>인코딩 과정</h3>
            <aside>
                <ol>
                    <li><strong>단어장(Vocabulary) 생성</strong> :<br> 전체 텍스트에서 고유한 단어들을 모두 모아 '단어:고유번호' 형태의 <strong>사전(Dictionary)</strong>을 만듭니다.</li>
                    <li><strong>인덱싱(Indexing)</strong> :<br> 단어장의 각 단어에 0부터 시작하는 고유 정수(Index)를 할당합니다.</li>
                    <li><strong>벡터 변환</strong> :<br> 단어장 크기와 동일한 차원의 영벡터(zero vector)를 생성한 뒤,<br>해당 단어의 인덱스 위치 값만 1로 변경합니다.</li>
                </ol>
            </aside>

            <h3>NLP에서의 한계</h3>
            <aside>
                <ol>
                    <li><strong>차원의 저주와 희소성 (Sparsity)</strong> :<br> 단어 수가 수만 개에 이르면 벡터의 차원도 그만큼 커집니다. 대부분의 값이 0이므로 메모리 낭비가 심하고 계산이 비효율적입니다.</li>
                    <li><strong>의미 관계 표현 불가</strong> :<br> 모든 단어 벡터가 서로 직교(orthogonal)하므로, '노트북'과 '컴퓨터'가 '지우개'보다 의미적으로 가깝다는 유사성을 전혀 표현할 수 없습니다.</li>
                </ol>
            </aside>
        </section>

        <p class="transition-text fade-in-up">이러한 한계점을 극복하기 위해 등장한 것이 <strong>워드 임베딩(Word Embedding)</strong> 기술입니다.</p>
        
        <div class="section-title fade-in-up">
            <h2>2. 워드 임베딩 (Word Embedding)</h2>
        </div>
        
        <section class="content-block card fade-in-up">
            <p>워드 임베딩은 원-핫 인코딩의 한계를 극복하기 위해 등장한 기술로, 각 단어를 저차원의 <strong>밀집 벡터(Dense Vector)</strong>로 표현합니다. 이 벡터는 단어의 문법적, 의미적 정보를 압축하여 담고 있습니다.</p>
             <aside>
                <h4>핵심 개념</h4>
                <ul>
                    <li><strong>분산 표현 (Distributed Representation)</strong> :<br> 단어의 의미를 벡터의 여러 차원에 걸쳐 분산시켜 표현합니다.</li>
                    <li><strong>의미적 유사도</strong> :<br> 의미가 비슷한 단어들은 벡터 공간상에서 서로 가까운 위치에 존재하게 됩니다. <br>이를 통해 모델은 단어 간의 관계를 학습하고 추론할 수 있습니다.</li>
                </ul>
            </aside>
        </section>

        <div class="model-compare-grid">
             <div class="model-card card highlight-gray fade-in-up">
                <h3>원-핫 인코딩(One-hot Encoding)</h3>
                <ul>
                    <li><strong>벡터 타입</strong> : 희소 벡터 (Sparse)</li>
                    <li><strong>차원</strong> : 고차원 (단어장 크기)</li>
                    <li><strong>의미 관계</strong> : 표현 불가</li> 
                </ul>
             </div>
             <div class="model-card card fade-in-up">
                <h3>워드 임베딩(Word Embedding)</h3>
                <ul>
                    <li><strong>벡터 타입</strong> : 밀집 벡터 (Dense)</li>
                    <li><strong>차원</strong> : 저차원 (사용자 지정)</li>
                    <li><strong>의미 관계</strong> : 표현 가능</li>
                </ul>
             </div>
        </div>


        <div class="section-title fade-in-up">
            <h2>NLP 데이터 처리의 큰 그림 : 공장 생산 라인</h2>
            <p>AI 모델을 학습시키는 과정은 원재료(텍스트)를 가공하여<br> 최종 제품(학습된 모델)을 만드는 공장과 같습니다.</p>
        </div>
        
        <div class="steps-grid">
            <article class="step-card card fade-in-up">
                <div class="step-number">1</div>
                <div class="step-card-content">
                    <h3>1단계 : Preprocessing & Tokenization</h3>
                    <ul>
                        <li><p><strong>실행</strong><br> : 텍스트를 들여와 노이즈를 <strong>제거(클리닝)</strong>하고, 일정한 크기로 잘라 토큰화를 합니다.</p></li>
                        <li><p><strong>결과물</strong><br> : ['나', '는', 'NLP', '를', '공부', '한다'] → 토큰 리스트</p></li>
                    </ul>
                </div>
            </article>

            <article class="step-card card fade-in-up">
                <div class="step-number">2</div>
                <div class="step-card-content">
                    <h3>2단계 : Vectorization (벡터화)</h3>
                    <p>리스트의 토큰을 모델이 학습 할 수 있게 벡터로 변환하는 과정</p>
                    <div class="sub-section">
                        <h4>첫번째 방법 : 원-핫 인코딩 (One-hot Encoding)</h4>
                        <p>원-핫 인코딩은 각 단어를 어휘 집합(Vocabulary)의 전체 크기와 동일한 차원을 갖는 벡터로<br> 표현하는 방식입니다. 이 벡터는 표현하려는 <strong>단어의 고유한 인덱스 위치만 1</strong>이고, <strong>나머지 모든 위치의 값은 0</strong>으로 채워져 있습니다.</p>
                        <p><strong>특징</strong></p>
                        <ul>
                            <li><strong>고차원 / 희소 벡터 (Sparse Vector)</strong> :<br> 벡터의 길이가 단어장의 전체 크기와 같아 매우 길고(고차원), 단 하나의 값을 제외한 모든 값이 0으로 채워져 있어 (희소성) 메모리 및 계산 측면에서 비효율적입니다.</li>
                            <li><strong>의미 관계 표현 불가</strong> :<br> 모든 단어 벡터는 서로 독립적(orthogonal)입니다. 이 때문에 '컴퓨터'와 '노트북' 벡터 간의 수학적 관계나 '컴퓨터'와 '사과' 벡터 간의 관계에 아무런 차이가 없습니다. 즉, 벡터만으로는 단어 간의 의미적 유사성을 전혀 계산할 수 없습니다.</li>
                        </ul>
                    </div>
                     <div class="sub-section">
                        <h4>두번째 방법 : 워드 임베딩 (Word Embedding)</h4>
                        <p>워드 임베딩은 각 단어를 다차원 공간의 '좌표'로 표현하는 기술 <br>즉, 사용자가 지정한 크기(예: 100~300차원)의 짧은 벡터로 표현하는 방식입니다. 이 벡터는 0과 1로만 이루어진 것이 아니라, <strong>의미를 나타내는 실수(예: 0.12, -0.8, 0.66...)</strong>들로 채워져 있습니다.</p>
                        <p>가장 중요한 점은, 이 숫자들은 단어가 문장에서 사용되는 방식(문맥)을 학습하여 의미가 비슷한 단어는 서로 비슷한 숫자 구성을 갖게 된다는 것입니다.</p>
                        <p><strong>특징</strong></p>
                         <ul>
                            <li><strong>저차원/밀집 벡터 (Dense Vector)</strong> :<br> 벡터의 길이가 비교적 짧고, 모든 숫자가 의미 정보를 가지고 있어 효율적입니다.</li>
                            <li><strong>의미 관계 표현</strong> :<br> '노트북'과 '컴퓨터'는 서로 비슷한 벡터 값을 갖게 됩니다. 따라서 벡터 간의 거리를 계산하면 두 단어가 의미적으로 가깝다는 것을 알 수 있습니다.</li>
                        </ul>
                    </div>
                </div>
            </article>
            
            <article class="step-card card fade-in-up">
                <div class="step-number">3</div>
                <div class="step-card-content">
                    <h3>3-4단계 : Data Loading & Model Training</h3>
                     <p>미니배치(Minibatch)는 전체 학습 데이터셋을 여러 개의 작은 묶음으로 나눈 것입니다. <br>학습 시에 데이터 전체가 아닌, 이 미니배치 단위로 데이터를 입력받아 가중치를 업데이트합니다.</p>
                    <aside>
                        <h3>미니배치의 역할과 목적</h3>
                        <p>미니배치를 사용하는 이유는 크게 계산 효율성과 학습 안정성 두 가지입니다.</p>
                        <ul>
                            <li><strong>계산 효율성 (Computational Efficiency)</strong> :<br> GPU와 같은 하드웨어는 여러 데이터를 동시에 처리하는 병렬 연산, 특히 행렬(Matrix) 연산에 최적화되어 있습니다. 여러 개의 데이터 샘플(벡터)을 하나의 미니배치(행렬/텐서)로 묶어서 전달하면, GPU는 병렬 처리를 통해 훨씬 빠른 속도로 연산을 수행할 수 있습니다.</li>
                            <li><strong>학습 안정성 (Training Stability)</strong> :<br> 모델은 데이터의 예측 오차를 기반으로 가중치를 얼마나 수정할지(그래디언트, Gradient)를 계산하며 학습합니다.
                                <ul>
                                    <li><strong>데이터 1개만 사용</strong> :<br> 1개의 데이터로 계산된 그래디언트는 매우 불안정하고 노이즈가 심해, 학습 방향이 크게 흔들릴 수 있습니다.</li>
                                    <li><strong>전체 데이터 사용</strong> :<br> 전체 데이터로 계산된 그래디언트는 가장 정확하지만, 데이터가 클 경우 계산에 너무 많은 시간과 메모리가 소요됩니다.</li>
                                    <li><strong>미니배치 사용</strong> :<br> 미니배치로 계산된 그래디언트는 전체 데이터의 그래디언트에 대한 좋은 근사치(approximation) 역할을 합니다. 전체 데이터를 사용하는 것보다 훨씬 빠릅니다. 이는 모델이 더 안정적이고 효율적으로 최적의 가중치를 찾아가도록 돕습니다.</li>
                                </ul>
                            </li>
                        </ul>
                    </aside>
                </div>
            </article>

        </div>

    </main>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const animatedElements = document.querySelectorAll('.fade-in-up');

            if ('IntersectionObserver' in window) {
                const observer = new IntersectionObserver((entries) => {
                    entries.forEach(entry => {
                        if (entry.isIntersecting) {
                            entry.target.classList.add('visible');
                            observer.unobserve(entry.target);
                        }
                    });
                }, {
                    threshold: 0.1
                });

                animatedElements.forEach(el => {
                    observer.observe(el);
                });
            } else {
                animatedElements.forEach(el => {
                    el.classList.add('visible');
                });
            }
        });
    </script>

</body>
</html>
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>텍스트 분류(Text Classification) 정리</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@400;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Noto Sans KR', sans-serif;
            background-color: #e9ecef;
            color: #212529;
            line-height: 1.7;
            margin: 0;
            padding: 40px 20px;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
        }

        h1, h2, h3, h4 {
            margin-top: 0;
            margin-bottom: 0.5rem;
            font-weight: 700;
        }

        h1 {
            font-size: 2.5rem;
            text-align: center;
            margin-bottom: 0.5rem;
        }

        h2 {
            font-size: 2rem;
            text-align: center;
            margin-top: 3rem;
            margin-bottom: 1.5rem;
        }
        
        h3 {
            font-size: 1.75rem;
            margin-bottom: 1rem;
        }
        
        h4 {
            font-size: 1.25rem;
            margin-bottom: 0.75rem;
        }

        p {
            margin-top: 0;
            margin-bottom: 1rem;
        }
        
        p:last-child {
            margin-bottom: 0;
        }

        .subtitle {
            text-align: center;
            color: #6c757d;
            font-size: 1.25rem;
            margin-bottom: 2.5rem;
        }
        
        hr {
            border: 0;
            height: 1px;
            background-color: #dee2e6;
            margin: 4rem 0;
        }

        .card {
            border: 1px solid #dee2e6;
            border-radius: 0.5rem;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05);
            margin-bottom: 1.5rem;
            padding: 2rem;
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }
        
        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 8px 15px rgba(0, 0, 0, 0.08);
        }

        .conceptual-card {
            background-color: #f8f9fa;
        }
        
        .sub-card {
            background-color: #ffffff;
            border: 1px solid #e9ecef;
            border-radius: 0.375rem;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.04);
            padding: 1.5rem;
            margin-top: 1rem;
        }

        pre {
            background-color: #f5f7fa;
            border: 1px solid #e9ecef;
            border-radius: 0.25rem;
            padding: 1rem;
            white-space: pre-wrap;
            word-wrap: break-word;
            font-size: 0.9em;
        }

        b, strong {
            font-weight: 700;
        }
        
        .section-title {
            margin-bottom: 1.5rem;
        }

        .footer-section {
            padding-top: 1rem;
        }
        
    </style>
</head>
<body>
    <div class="container">
        <h1>텍스트 분류(Text Classification) 정리</h1>
        <p class="subtitle">
            텍스트 분류는 "이 영화 진짜 재밌다!" 라는 문장을 <b>'긍정'</b>으로,
            <br> "시간 아까웠음" 이라는 문장을 <b>'부정'</b>으로 컴퓨터가 알아서 분류하게 만드는 것입니다. 
            <br>이 목표를 위해 딥러닝 모델이 어떤 사고의 과정을 거치는지 단계별로 따라가 보겠습니다.
        </p>
        
        <h2>1단계 : 컴퓨터가 이해하는 숫자로 변환하기</h2>
        <div class="card conceptual-card">
            <h3>단어를 숫자로 표현하기</h3>
            <p>컴퓨터는 '재미있다'라는 단어의 의미를 모릅니다. 오직 숫자만 이해할 뿐이죠. 
            <br>그래서 모든 자연어 처리의 첫 번째 관문은 <b>"단어를 어떻게 숫자로 바꿀 것인가?"</b> 입니다.</p>
            
            <div class="sub-card">
                <h4>첫 번째 시도 : 원-핫 벡터 (One-hot Vector)</h4>
                <p>가장 단순한 방법은 세상의 모든 단어에 번호를 매기고, 
                <br><b>해당 단어의 번호만 1로 표시하고 나머지는 모두 0으로 채우는 겁니다.</b></p>
                <pre><b>단어장이 [나, 는, 영화, 본다] 라면, '영화'는 [0, 0, 1, 0] 이 됩니다.</b></pre>
                <p>하지만 이 방법엔 치명적인 단점이 있습니다.</p>
                <p>
                    <strong>1.차원의 저주</strong>
                    <br>
                    단어가 10,000개면 10,000차원 벡터가 필요해요. 너무 비효율적입니다.
                </p>
                 <p>
                    <strong>2. 의미 실종</strong>
                    <br>
                    '영화' [0,0,1,0] 와 '영사기' [...1...] 는 의미적으로 관련이 깊지만, 
                    <br>원-핫 벡터 상에서는 아무런 관계가 없는, 그냥 서로 다른 숫자일 뿐입니다.
                </p>
            </div>
            
            <div class="sub-card">
                <h4>더 나은 해결책 "임베딩 벡터 (Embedding Vector)"</h4>
                <p>이 문제를 해결하기 위해 <b>"단어의 의미를 다차원 공간의 좌표" 로 표현하는 '임베딩'</b>이 등장합니다.</p>
                <pre><b>'왕' 이라는 단어는 (0.8, 0.2, 0.9)
'여왕' 이라는 단어는 (0.7, 0.9, 0.8)
'사과' 라는 단어는 (-0.5, 0.6, -0.4)</b></pre>
                <p>이런 식으로 표현하는 거죠. 이렇게 하면 놀라운 일이 벌어집니다. 
                <br>의미가 비슷한 단어들은 공간상에서 가까운 곳에 위치하게 돼요. 
                <br>이제 모델은 단어 간의 관계(예: 왕 - 남자 + 여자 = 여왕)를 벡터 연산을 통해 학습할 수 있게 됩니다.</p>
                <p>텍스트를 처리하기 위해, 우리는 의미가 없는 원-핫 벡터 대신, 단어의 의미와 관계를 함축한 저차원의 <b>'임베딩 벡터'</b>를 사용합니다. 이것이 바로 모델의 첫 번째 층인 <b>임베딩 레이어(Embedding Layer)</b>의 역할입니다.</p>
            </div>
        </div>

        <hr>

        <h2>2단계 : 단어의 '순서'와 '문맥'을 파악하기 (RNN의 역할)</h2>
        <div class="card conceptual-card">
            <h3>순서와 문맥 파악하기</h3>
            <p>"나는 너를 사랑해"와 "너는 나를 사랑해"는 같은 단어로 이루어져 있지만 의미가 다릅니다. 
            <br>바로 <b>'순서'</b> 때문입니다. <b>RNN(Recurrent Neural Network)</b> 은 이 '순서'와 '문맥'을 파악하는 아키텍쳐입니다.</p>
            
            <div class="sub-card">
                <h4>RNN의 작동 방식</h4>
                <p>RNN은 단어 임베딩 벡터를 하나씩 순서대로 입력받습니다. 
                <br>그리고 각 단어를 볼 때마다 내부적으로 <b>'기억' (Hidden State)</b> 을 계속 업데이트해요.</p>
                <pre>"나는" 이라는 단어를 보고 기억을 업데이트 <b>👉 기억 ver 1.0</b>
"너를" 이라는 단어를 보고, <b>기억 ver 1.0</b>을 참고하여 기억을 업데이트 <b>👉 기억 ver 2.0</b>
"사랑해" 라는 단어를 보고, <b>기억 ver 2.0</b>을 참고하여 기억을 업데이트 <b>👉 기억 ver 3.0 (최종)</b></pre>
                <p>이 <b>최종 기억(기억 ver 3.0)</b>에는 문장 전체의 정보가 압축되어 담겨있다고 보는 겁니다.</p>
            </div>

            <div class="sub-card">
                <h4>더 나아가기 : 다층 양방향 RNN</h4>
                <p>
                    <h4>양방향 (Bi-directional)</h4>
                    "그 영화의 주인공은 연기를 못했다" 라는 문장에서 '주인공'의 의미를 정확히 파악하려면 뒤에 나오는 '못했다'라는 정보가 중요해요. 
                    그래서 <b>문장을 앞에서 뒤로만 읽는 게 아니라, 뒤에서 앞으로도 읽어서 양쪽의 문맥을 모두 합쳐</b> 훨씬 풍부한 이해를 가능하게 합니다.
                </p>
                <p>
                    <h4>다층 (Multi-layered)</h4>
                    RNN 층을 여러 개 쌓는 겁니다. 1층 RNN이 기본적인 문맥을 파악하면, 2층 RNN은 그 문맥을 바탕으로 더 추상적이고 고차원적인 특징을 학습하게 됩니다.
                </p>
                <p>RNN은 순차적인 데이터(문장)를 처리하며 문맥 정보를 <b>'은닉 상태(Hidden State)'</b>에 압축합니다. 
                <br>특히 <b>양방향, 다층 구조</b>를 통해 훨씬 더 정교하게 문장의 의미를 파악할 수 있습니다.</p>
            </div>
        </div>

        <hr>

        <h2>3단계 : 종합해서 결론 내리기 (분류 과정)</h2>
        <div class="card conceptual-card">
            <h3>Feed-forward(순전파) 과정</h3>
            <p>이제 모든 재료가 준비되었습니다. 모델이 최종적으로 "이 문장은 '긍정'이야!" 라고 판단하는 전체 과정을 정리해 봅시다.</p>
            <div class="sub-card">
                <p>
                    <strong>입력</strong> : <b>"이 영화 진짜 재밌다"</b> 라는 문장이 들어옵니다. (정확히는 <b>단어 번호의 배열</b>)
                </p>
                <p>
                    <strong>1. 임베딩</strong>
                    <br>
                    각 단어("이", "영화", "진짜", "재밌다")가 의미를 담은 <b>임베딩 벡터로 변환</b>됩니다.
                </p>
                <p>
                    <strong>2. RNN 처리</strong>
                    <br>
                    임베딩 벡터들이 <b>순서대로 양방향, 다층 RNN에 입력됩니다.</b> 
                    <br>RNN은 문장을 앞에서부터, 그리고 뒤에서부터 꼼꼼히 읽으며 문맥 전체를 압축한 최종 정보 벡터를 만들어냅니다.
                </p>
                <p>
                    <strong>3. 정보 선택 (Slicing)</strong>
                    <br>
                    RNN은 사실 모든 단어를 읽을 때마다 중간 결과물을 내놓습니다. 
                    하지만 우리는 문장 전체를 요약한 정보가 필요하므로, 가장 마지막 단어까지 읽었을 때의 <b>최종 결과물만 딱 잘라내서 사용</b>합니다.
                </p>
                <p>
                    <strong>4. 분류 (Softmax)</strong>
                    <br>
                    이 최종 정보 벡터를 Softmax라는 분류기에 넣습니다. 
                    <br>Softmax는 이 벡터를 보고 '긍정'일 확률 95%, '부정'일 확률 5% 와 같이 각 클래스별 확률 값으로 변환해 줍니다.
                </p>
                <p>
                    <strong>결론</strong> : 가장 확률이 높은 '긍정'을 최종 예측 결과로 내놓습니다. 
                </p>
            </div>
        </div>
        
        <hr>

        <h2>번외 : 현실적인 문제와 똑똑한 해결책</h2>
        <div class="card conceptual-card">
             <h3>가변 길이 데이터 처리하기</h3>
             <p>이론적으로는 완벽해 보이지만, 실제 데이터를 다룰 땐 한 가지 귀찮은 문제가 있었습니다.
             <br>바로 <b>"문장마다 길이가 다르다"</b>는 점입니다. 모델은 고정된 크기의 묶음(미니배치)을 처리해야 하는데, 길이가 제각각인 문장들을 어떻게 하나의 묶음으로 만들 수 있을까요?</p>
             <div class="sub-card">
                 <h4>해결책 : 패딩(Padding)과 콜레이트 함수(Collate Function)</h4>
                 <p>정답은 "가장 긴 문장을 기준으로 짧은 문장의 뒤에 의미 없는 숫자(예: 0)를 붙여 길이를 맞추는 것", 
                 즉 <b>패딩(Padding)</b> 입니다. 이때, 데이터 로더(DataLoader)가 미니배치를 구성할 때 이 <b>패딩 작업을 동적으로 처리</b>해주는 아주 똑똑한 일꾼이 바로 <b>콜레이트 함수(Collate Function)</b> 입니다.</p>
                 <p>DataLoader는 Dataset에서 문장들을 하나씩 가져와 리스트 [문장1, 문장2, 문장3] 를 만듭니다. <br>이 리스트를 collate_fn에게 넘겨주면, collate_fn은 이 리스트 안에서 가장 긴 문장을 찾아, 나머지 짧은 문장들 뒤에 패딩을 붙여 모두 같은 길이로 만든 뒤, 하나의 커다란 텐서 덩어리로 합쳐서 모델에게 전달합니다. 덕분에 우리는 문장 길이에 신경 쓰지 않고 효율적으로 모델을 학습시킬 수 있게 됩니다.</p>
                 <p>가변 길이의 문장들을 묶어 처리하기 위해 <b>패딩(Padding)</b> 이라는 기법을 사용하며, 이 작업은 <b>DataLoader</b> 내부의 collate_fn이 각 미니배치마다 동적으로 수행해 줍니다.</p>
             </div>
        </div>
    </div>
</body>
</html>
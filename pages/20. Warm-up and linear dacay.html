<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>트랜스포머 학습 안정화 기법</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@400;500;700&display=swap" rel="stylesheet">
    
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <style>
        body {
            font-family: 'Noto Sans KR', sans-serif;
            background-color: #f7f8fc;
            color: #495057;
            margin: 0;
            padding: 0;
            line-height: 1.7;
            -webkit-font-smoothing: antialiased;
        }

        .container {
            max-width: 900px;
            margin: 40px auto;
            padding: 0 20px;
        }

        h1 {
            font-size: 2.5rem;
            font-weight: 700;
            color: #212529;
            text-align: center;
            margin-bottom: 12px;
        }

        p.subtitle {
            font-size: 1.15rem;
            color: #6c757d;
            text-align: center;
            margin-top: 0;
            margin-bottom: 40px;
        }

        h2 {
            font-size: 1.75rem;
            font-weight: 700;
            color: #212529;
            text-align: center;
            margin: 60px 0 30px 0;
        }

        h3 {
            font-size: 1.4rem;
            font-weight: 700;
            color: #212529;
            margin-top: 0;
            margin-bottom: 15px; /* Increased bottom margin */
        }

        h4 {
            font-size: 1.1rem;
            font-weight: 700;
            color: #343a40;
            margin-top: 0;
            margin-bottom: 12px;
        }

        p {
            margin-top: 0;
            margin-bottom: 1rem;
            color: #495057;
        }
        
        p.secondary {
             color: #6c757d;
             font-size: 1.05rem;
        }

        /* Use <b> tag directly in HTML for emphasis */
        b { 
            font-weight: 700;
            color: #343a40;
        }

        hr {
            border: 0;
            height: 1px;
            background-color: #dee2e6;
            margin: 60px 0;
        }

        .card {
            border: 1px solid #e9ecef;
            border-radius: 12px;
            margin-bottom: 24px;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.04);
            overflow: hidden;
        }

        .conceptual-card {
            background-color: #f8f9fa;
            padding: 24px 30px;
            transition: transform 0.2s ease, box-shadow 0.2s ease;
        }

        .conceptual-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 8px 20px rgba(0, 0, 0, 0.08);
        }

        .sub-card {
            background-color: #ffffff;
            border: 1px solid #e9ecef;
            border-radius: 8px;
            padding: 20px 24px;
            margin-top: 20px;
            box-shadow: 0 2px 6px rgba(0, 0, 0, 0.03);
        }
        
        .side-by-side-container {
            display: flex;
            gap: 20px;
            margin-top: 20px;
        }

        .side-by-side-container > .sub-card {
            flex: 1;
            margin-top: 0;
        }
        
        pre {
            background-color: #f5f7fa;
            border: 1px solid #e0e6ed;
            border-radius: 6px;
            padding: 16px;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
            font-size: 0.9rem;
            line-height: 1.6;
            overflow-x: auto;
            margin-top: 10px;
            margin-bottom: 1rem;
        }
        
        @media (max-width: 768px) {
            .side-by-side-container {
                flex-direction: column;
            }
            h1 { font-size: 2rem; }
            h2 { font-size: 1.5rem; }
        }
    </style>
</head>
<body>

    <div class="container">

        <h1>트랜스포머 학습 안정화 기법</h1>
        <p class="subtitle">Adam 옵티마이저, Warm-up & Decay, Layer Normalization 이해하기</p>

        <hr>
        <h2>섹션 1: 모델 학습의 조력자, 옵티마이저 (Optimizer)</h2>

        <div class="card conceptual-card">
            <h3>최적점을 찾아가는 길잡이</h3>
            <p class="secondary">AI 모델, 특히 트랜스포머처럼 복잡한 모델을 학습시킨다는 것은, 결국 모델이 <b>가장 좋은 성능을 내는 파라미터(가중치) 값</b>을 찾아가는 과정입니다. 마치 산에서 가장 낮은 지점(최적점)을 찾아 내려가는 것과 같죠.</p>
            <p class="secondary">이때 <b>'옵티마이저(Optimizer)'</b>는 길잡이 역할을 합니다. 현재 위치에서 어느 방향으로, 얼마나 큰 보폭으로 가야 가장 빠르고 정확하게 최저점에 도달할 수 있을지 알려주는 알고리즘입니다.</p>
            
            <div class="sub-card">
                <h4>기본적인 방법 SGD (확률적 경사 하강법)</h4>
                <p>가장 기본적인 옵티마이저입니다. 현재 위치의 <b>기울기(경사)</b>를 보고, 가장 가파른 내리막 방향으로 정해진 <b>보폭(학습률, Learning Rate)</b>만큼 이동하는 방식입니다.</p>
                <p>단점은, 학습률을 너무 크게 잡으면 최저점을 지나쳐 버리고(발산), 너무 작게 잡으면 학습이 너무 느리다는 것입니다.</p>
            </div>
        </div>

        <hr>
        <h2>섹션 2: 똑똑한 길잡이, Adam (Adaptive Moment Estimation)</h2>

        <div class="card conceptual-card">
            <h3>스스로 보폭을 조절하는 옵티마이저</h3>
            <p class="secondary"><b>Adam</b>은 SGD의 단점을 개선한, 훨씬 똑똑하고 널리 쓰이는 옵티마이저입니다. 이름의 'Adaptive'에서 알 수 있듯이, 상황에 맞게 <b>스스로 학습률(보폭)을 조절</b>하는 능력이 있습니다.</p>
            
            <div class="sub-card">
                <h4>핵심 아이디어</h4>
                <p><b>1. 관성(Momentum) 활용</b><br>
                이전에 이동했던 방향을 기억해서, 현재 기울기 방향과 합쳐 다음 이동 방향을 결정합니다. (내리막길에서 속도가 붙는 것처럼) 이를 통해 더 빠르고 안정적으로 이동할 수 있습니다.</p>
                 <p><b>2. 맞춤형 보폭(Adaptive Learning Rate)</b><br>
                각 파라미터마다 '지금까지 얼마나 자주, 얼마나 크게 변했는지'를 추적합니다. 자주 변했던 파라미터는 보폭을 줄이고, 거의 변하지 않았던 파라미터는 보폭을 늘리는 식으로, <b>파라미터별로 최적의 학습률을 자동으로 조절</b>합니다. (RMSprop 개념 활용)</p>
            </div>
            <div class="sub-card">
                 <h4>장점</h4>
                 <p>일반적으로 SGD보다 빠르고 안정적으로 최적점을 찾아가는 경향이 있어 많은 딥러닝 모델 학습에 기본적으로 사용됩니다.</p>
            </div>
        </div>

        <hr>
        <h2>섹션 3: 트랜스포머와 Adam의 만남 - 초기 불안정성 문제</h2>

        <div class="card conceptual-card">
            <h3>까다로운 트랜스포머 학습</h3>
             <p class="secondary">Adam은 정말 좋은 옵티마이저지만, 트랜스포머처럼 <b>아주 깊고 복잡한 네트워크</b>에서는 학습 <b>초반</b>에 문제를 일으키는 경우가 있습니다.</p>
             
             <div class="sub-card">
                 <h4>문제점</h4>
                 <p><b>1. 불안정한 초기 기울기</b><br>
                 학습 시작 시점에는 모델 파라미터가 무작위 상태라, 기울기(Gradient) 값이 매우 크고 불안정하게 튈 수 있습니다.</p>
                 <p><b>2. Adam의 과민 반응</b><br>
                 Adam의 똑똑한 '적응형 학습률' 기능이 이 불안정한 초기 기울기에 너무 민감하게 반응하여, 잘못된 방향으로 너무 큰 보폭을 내딛거나, 잘못된 '관성(Momentum)'을 쌓아버릴 수 있습니다.</p>
                  <p><b>3. 결과</b><br>
                 모델이 최적점이 아닌 이상한 지점(Bad local optima)에 너무 빨리 빠져버리거나, 학습 자체가 불안정해져 성능이 잘 나오지 않는 문제가 발생합니다.</p>
            </div>
             <p class="secondary">즉, Adam 자체의 문제라기보다는, <b>Adam의 좋은 기능이 트랜스포머의 초기 불안정성과 만나 문제를 일으키는 것</b>입니다.</p>
        </div>

        <hr>
        <h2>섹션 4: 해결책 1 - 학습률 조절 마법, 'Warm-up and Decay'</h2>

        <div class="card conceptual-card">
            <h3>학습 초반 안정화 전략</h3>
            <p class="secondary">이 초기 불안정성 문제를 해결하는 가장 대표적인 방법이 바로 학습률(Learning Rate, LR)을 스케줄링하는 <b>'Warm-up and Decay'</b> 기법입니다.</p>

            <div class="sub-card">
                <h4>핵심 아이디어</h4>
                <p>학습 초반에는 아주 조심스럽게 시작해서 모델이 안정될 시간을 주고, 안정된 후에는 본격적으로 학습하다가, 막판에는 다시 세밀하게 조정하자는 것입니다.</p>
                <p></p>
            </div>

            <div class="sub-card">
                <h4>작동 방식</h4>
                <p><b>1. Warm-up (준비 운동)</b><br>
                학습 시작 시, 학습률을 0에 가까운 아주 작은 값에서 시작하여, 미리 정해둔 특정 스텝 수('#warm-up steps')까지 <b>선형적으로(점진적으로) 목표 학습률(LR)까지 증가</b>시킵니다.</p>
                <p><b>효과</b> 학습 초반 불안정한 기울기에 Adam이 과민 반응하지 않도록 '워밍업' 시간을 줍니다. 모델이 안정적인 방향을 잡을 때까지 작은 보폭으로 탐색하게 하여, 잘못된 Momentum이 쌓이는 것을 방지합니다.</p>
                
                <p><b>2. Decay (감소)</b><br>
                Warm-up이 끝난 후 목표 학습률에 도달하면, 그 이후부터는 학습률을 점진적으로 <b>다시 감소</b>시킵니다(보통 선형적 또는 코사인 함수 형태).</p>
                <p><b>효과</b> 학습 후반부, 모델이 최적점에 가까워졌을 때 보폭을 줄여서 최적점을 지나치지 않고 세밀하게 수렴하도록 돕습니다.</p>
            </div>
            
            <div class="sub-card">
                 <h4>어려움: 하이퍼파라미터 튜닝</h4>
                 <p>하지만 이 방법은 '#warm-up steps', '초기 LR', '총 학습 스텝 수(#total iterations)', '배치 크기(Batch Size)' 등 <b>새로운 하이퍼파라미터</b>를 또 맞춰야 하는 어려움이 있습니다. 데이터셋과 모델 크기가 클수록 이 값을 찾는 비용(시간, 자원)이 매우 비싸기 때문에, 많은 연구자들이 실험적으로 찾아놓은 '적당한 값'(예: LR 1e-5 근처, Warm-up 1000 스텝)을 가져다 쓰거나, HuggingFace의 <b>TrainerArguments</b> 같은 도구를 활용하는 경우가 많습니다.</p>
            </div>
        </div>

        <hr>
        <h2>섹션 5: 해결책 2 - 구조 변경 마법, 'Layer Normalization' 위치 변경</h2>

        <div class="card conceptual-card">
            <h3>학습 안정성을 높이는 구조적 변경</h3>
            <p class="secondary">또 다른 해결책은 옵티마이저나 학습률을 건드리는 대신, <b>트랜스포머 모델 구조 자체를 약간 변경</b>하여 학습 안정성을 높이는 것입니다. 바로 <b>'Layer Normalization (LN)'</b>의 위치를 바꾸는 <b>'Pre-LN'</b> 방식입니다.</p>

            <div class="sub-card">
                 <h4>Layer Normalization (LN)이란?</h4>
                 <p>모델의 각 층(Layer)을 통과하는 데이터(신호)의 분포를 안정화(정규화)시켜주는 기술입니다. 학습을 더 빠르고 안정적으로 만들어주는 중요한 부품 중 하나입니다.</p>
            </div>
            
            <div class="side-by-side-container">
                <div class="sub-card">
                    <h4>기존 방식 (Post-LN)</h4>
                    <p>원래 트랜스포머 논문에서는 어텐션이나 FFN 계산을 하고, 잔차 연결(Add)을 한 <b>뒤에(Post)</b> LN을 적용했습니다. ( MHA → Add → <b>LN</b> )</p>
                    <p><b>문제점</b> 이 방식은 학습 초반에 기울기가 불안정해지는 경향이 있어, LR Warm-up이 거의 필수적이었습니다.</p>
                    <p></p>
                </div>

                <div class="sub-card">
                    <h4>제안된 방식 (Pre-LN)</h4>
                    <p>LN을 계산 <b>전에(Pre)</b>, 즉 잔차 연결 직전에 적용하는 방식입니다. ( <b>LN</b> → MHA → Add )</p>
                    <p><b>효과</b> 데이터(신호)를 먼저 안정화시킨 후 계산을 진행하므로, 기울기(Gradient)가 더 안정적으로 흐르고 학습 과정이 전반적으로 부드러워집니다.</p>
                     <p><b>장점</b> 실험 결과를 보면, Pre-LN 방식이 Post-LN 방식보다 더 빨리 안정되고 높은 성능을 보이는 경우가 많으며, <b>LR Warm-up 없이도 비교적 안정적인 학습</b>이 가능해지는 효과를 보여줍니다.</p>
                     <p></p>
                </div>
            </div>
        </div>

        <hr>
        <h2>섹션 6: 결론 및 요약</h2>

        <div class="card conceptual-card">
            <h3>트랜스포머 학습 안정화 전략 종합</h3>
            <p class="secondary">오늘 살펴본 내용을 종합하면 다음과 같습니다.</p>

             <div class="sub-card">
                 <p><b>1. Adam</b><br>
                 똑똑한 옵티마이저지만, 트랜스포머 학습 초반에는 불안정할 수 있다.</p>
             </div>
              <div class="sub-card">
                 <p><b>2. LR Warm-up & Decay</b><br>
                 Adam의 초기 불안정성을 잡아주는 효과적인 학습률 조절 기법이지만, 하이퍼파라미터 튜닝이 필요하다. (보통 Adam의 개선 버전인 <b>AdamW</b>와 함께 많이 사용됨)</p>
             </div>
              <div class="sub-card">
                 <p><b>3. Pre-LN</b><br>
                 트랜스포머 구조 자체를 변경하여 학습 안정성을 높이는 방법. Warm-up의 필요성을 줄여줄 수 있다.</p>
             </div>
              <div class="sub-card">
                 <p><b>4. 현실</b><br>
                 Pre-LN 구조가 더 안정적이지만, 여전히 LLM 등에서는 <b>성능상의 이유나 기존 연구와의 일관성</b> 등을 위해 Post-LN 구조와 LR Warm-up 조합도 계속 사용되고 있다. (즉, 상황에 따라 선택)</p>
             </div>
             
             <p class="secondary">결국, 트랜스포머 학습은 다소 까다롭기 때문에, 안정적인 학습을 위해 Adam 옵티마이저와 함께 LR Warm-up 기법을 사용하거나, Pre-LN 구조를 채택하는 등의 다양한 방법들이 연구되고 적용되고 있습니다. 이 외에도 가중치 초기화(Weight Initialization) 방법 등 다른 요소들도 학습 안정성에 영향을 줄 수 있습니다.</p>
        </div>

    </div>

</body>
</html>
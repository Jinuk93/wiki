<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>워드 임베딩(Word Embedding)이란?</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@400;700;900&display=swap" rel="stylesheet">
    <style>
        /* 1. Core Design Philosophy & Technical Specs */
        :root {
            --color-text-primary: #212529;
            --color-text-secondary: #6c757d;
            --color-border: #dee2e6;
            --color-bg-white: #ffffff;
            /* ▼▼▼ [수정된 부분] 색상을 조금 더 진하게 변경 ▼▼▼ */
            --color-bg-subtle: #e9ecef; /* 연한 회색 */
            --color-highlight-orange-bg: #fff0db; /* 연한 오렌지 */
            --color-highlight-orange-border: #ffe2b8;
            /* ▲▲▲ [수정된 부분] 색상을 조금 더 진하게 변경 ▲▲▲ */
            --font-family-base: 'Noto Sans KR', sans-serif;
        }

        html { scroll-behavior: smooth; }
        body { font-family: var(--font-family-base); font-size: 1rem; line-height: 1.7; background-color: var(--color-bg-white); color: var(--color-text-primary); margin: 0; padding: 4rem 1rem; -webkit-font-smoothing: antialiased; }

        /* 2. Typography & Hierarchy */
        h1, h2, h3, h4, h5 { font-weight: 700; line-height: 1.3; margin-top: 0; }
        h1 { font-size: 2.5rem; font-weight: 900; margin-bottom: 0.5rem; }
        h2 { font-size: 2rem; margin-bottom: 0.75rem; }
        h3 { font-size: 1.5rem; margin-top: 2.5rem; margin-bottom: 1rem; padding-bottom: 0.75rem; border-bottom: 1px solid var(--color-border); }
        h4 { font-size: 1.2rem; margin-top: 2rem; margin-bottom: 1rem; }
        h5 { font-size: 1rem; margin-top: 1.5rem; margin-bottom: 0.5rem; color: var(--color-text-secondary); }
        h3:first-of-type, h4:first-of-type, h5:first-of-type { margin-top: 0; }
        p { margin-top: 0; margin-bottom: 1rem; }
        strong, b { font-weight: 700; color: var(--color-text-primary); }
        code { font-family: Consolas, 'Courier New', monospace; background-color: #e9ecef; padding: 0.2em 0.4em; border-radius: 4px; font-size: 90%; }

        /* 3. Component Library & Structure */
        .container { max-width: 1200px; margin: 0 auto; }
        .header { text-align: center; margin: 0 auto 3rem auto; }
        .header p { font-size: 1.125rem; color: var(--color-text-secondary); max-width: 800px; margin: 0 auto; }
        
        .section-title { text-align: left; margin: 4rem 0 2rem 0; }
        .section-title p { color: var(--color-text-secondary); }

        .card { background-color: var(--color-bg-white); border: 1px solid var(--color-border); border-radius: 0.75rem; padding: 2.5rem; margin-bottom: 2rem; box-shadow: 0 4px 6px rgba(0, 0, 0, 0.02); transition: transform 0.3s ease, box-shadow 0.3s ease; text-align: left; }
        .card:hover { transform: translateY(-5px); box-shadow: 0 10px 20px rgba(0, 0, 0, 0.07); }
        
        .model-compare-grid { display: grid; grid-template-columns: 1fr; gap: 2rem; align-items: start; margin-top: 2rem; }
        @media (min-width: 1024px) { .model-compare-grid { grid-template-columns: 1fr 1fr; } }
        
        .model-group { border: 1px solid var(--color-border); border-radius: 0.75rem; padding: 2rem; }
        .model-group > h2 { font-size: 1.75rem; padding-bottom: 1rem; border-bottom: 2px solid var(--color-border); text-align: center; margin-bottom: 2rem; }
        
        .sub-card { background-color: #fff; border: 1px solid var(--color-border); border-radius: 0.5rem; padding: 1.5rem; margin-top: 1.5rem; box-shadow: 0 2px 4px rgba(0,0,0,0.02); }
        .sub-card:first-child { margin-top: 0; }
        .sub-card h3, .sub-card h4 { margin-top: 0; border-bottom: none; padding-bottom: 0; font-size: 1.3rem; }

        aside { background-color: var(--color-bg-white); border-left: 4px solid #ced4da; padding: 1rem 1.5rem; margin: 1.5rem 0; border-radius: 0 0.25rem 0.25rem 0; font-size: 0.95rem; }
        
        .highlight-gray { background-color: var(--color-bg-subtle); }
        .highlight-orange { background-color: var(--color-highlight-orange-bg); border-color: var(--color-highlight-orange-border); }
        .highlight-orange > h2 { border-color: var(--color-highlight-orange-border); }

        .fade-in-up { opacity: 0; transform: translateY(30px); transition: opacity 0.6s ease-out, transform 0.6s ease-out; }
        .fade-in-up.visible { opacity: 1; transform: translateY(0); }
    </style>
</head>
<body>

    <main class="container">
        <header class="header fade-in-up">
            <h1>워드 임베딩 (Word Embedding) 이란? 🧠</h1>
            <p>텍스트(단어)를 기계가 이해할 수 있는 의미가 담긴 <b>다차원 공간의 좌표(벡터)</b>로 표현하는 기술</p>
        </header>

        <section class="section-title fade-in-up">
            <h2>1. 문제의 시작 : 단순한 숫자 부여의 한계 (원-핫 인코딩)</h2>
        </section>

        <div class="content-block card highlight-gray fade-in-up">
            <p>워드 임베딩이 왜 필요한지 알려면, 과거에 사용되던 원-핫 인-코딩(One-Hot Encoding)의 한계를 먼저 이해해야 합니다. 이 방식은 단어장에 있는 모든 단어에 고유한 번호를 부여하고, 해당 번호의 위치만 1로, 나머지는 모두 0으로 표시하는 방식입니다.</p>
            <aside>
                <p><b>예시 :</b><br> 다음과 같은 단어장이 있다고 가정해 봅시다.<br>
                {'사과':0, '바나나':1, '컴퓨터':2, '노트북':3}</p>
                <p><b>사과</b> → [1, 0, 0, 0]<br>
                <b>바나나</b> → [0, 1, 0, 0]<br>
                <b>컴퓨터</b> → [0, 0, 1, 0]<br>
                <b>노트북</b> → [0, 0, 0, 1]</p>
            </aside>
            <p>이 방식은 두 가지 치명적인 문제를 가지고 있습니다.</p>
            <ol>
                <li style="margin-bottom: 1rem;"><b>모든 관계가 동등하다 (의미 표현 불가) :</b><br>원-핫 벡터들 간의 거리를 계산해 보면, 모든 단어는 서로에게 똑같이 '남남'입니다. '사과'와 '바나나'(둘 다 과일) 사이의 거리와 '사과'와 '컴퓨터'(전혀 다른 카테고리) 사이의 거리가 수학적으로 완전히 동일합니다. 즉, '사과'와 '바나나'가 '컴퓨터'보다 의미적으로 더 가깝다는 핵심 정보를 전혀 표현하지 못합니다.</li>
                <li><b>너무 거대한 주소 체계 (차원의 저주) : </b><br>단어장의 크기가 3만 개라면, 모든 단어는 3만 차원의 벡터가 됩니다. 숫자 대부분이 0이라 극심한 메모리 낭비와 계산 비효율을 초래합니다.</li>
            </ol>
        </div>

        <section class="section-title fade-in-up">
            <h2>2. 해결책 : 워드 임베딩의 종류와 발전 ⭐️</h2>
            <p>워드 임베딩은 <b>"비슷한 문맥에서 등장하는 단어는 비슷한 의미를 가질 것이다"</b>라는 아이디어를 바탕으로, 위 문제들을 해결하며 발전해 왔습니다.</p>
        </section>
        
        <div class="model-compare-grid">
            <div class="model-group highlight-gray fade-in-up">
                <h2>1. 정적 임베딩 (Static Embedding)</h2>
                <p><b>하나의 단어에 하나의 고정된 벡터값을 부여하는 방식입니다.</b> 마치 사전처럼, 단어의 의미를 모든 문맥에서 나타난 뜻의 '평균'으로 계산하여 하나의 좌표에 고정시킵니다.</p>
                
                <article class="sub-card">
                    <h4>작동 원리 (Lookup Table)</h4>
                    <p>미리 학습된 거대한 '단어-벡터' 표를 가지고, 단어의 인덱스 번호가 들어오면 해당하는 벡터를 꺼내주는 방식으로 작동합니다.</p>
                    <aside>
                        <p>단어 '컴퓨터' (인덱스: 2) → [조회] → <code>[0.9, 0.8, 0.2, ...]</code></p>
                    </aside>
                </article>

                <article class="sub-card">
                    <h4>주요 모델 비교</h4>
                    <ul>
                        <li><b>Word2Vec :</b> "주변 단어를 예측"하는 과정에서 단어의 의미를 학습합니다. (예: '맛있는', '먹는다'라는 단어들로 '사과'를 예측)</li>
                        <li><b>GloVe :</b> 말뭉치 전체에서 "단어들이 함께 등장한 횟수"를 기반으로 의미를 학습합니다.</li>
                        <li><b>FastText :</b> 단어를 "더 작은 단위(subword)로 분해"하여 학습해, 오타나 신조어에도 강건한 모습을 보입니다.</li>
                    </ul>
                </article>

                <article class="sub-card">
                    <h4>정적 임베딩의 한계 (동음이의어 문제) 🚧</h4>
                    <aside>
                        <p><b>'김민준'</b>이라는 이름을 가진 두 친구가 있다고 상상해 보세요. '의사 김민준'과 '요리사 김민준'이 있을 때, 정적 임베딩은 두 사람의 정보를 어설프게 섞어, <b>'의료와 요리에 모두 관심 있는 김민준'</b>이라는 애매한 정보 하나만 가지게 됩니다.</p>
                    </aside>
                </article>

                 <article class="sub-card">
                    <h4>어떤 TASK에서 주로 사용하는가? 🎯</h4>
                    <p>일반적으로 <b>빠른 속도가 중요하고, 문맥에 따른 미묘한 의미 변화가 중요하지 않은</b> Task에서 많이 사용합니다.</p>
                    <aside>
                        <h5>이커머스 (E-commerce) 🛍️</h5>
                        <p><b>주요 Task :</b> 실시간 상품 추천, 연관 검색어 확장<br>
                        <b>왜 지금도 쓸까요? :</b> BERT 같은 무거운 모델로 수백만 개 상품 간의 유사도를 실시간으로 계산하는 것은 매우 비싸고 느립니다. 미리 계산된 벡터로 빠르게 유사도를 찾는 Word2Vec 방식이 훨씬 효율적입니다.</p>
                    </aside>
                    <aside>
                        <h5>게임 산업 (Gaming) 🎮</h5>
                        <p><b>주요 Task :</b> 실시간 욕설/어뷰징 유저 탐지<br>
                        <b>왜 지금도 쓸까요? :</b> 게임 채팅 데이터는 양이 엄청나고 속도가 생명입니다. 또한, 게임 업계만의 은어와 신조어가 많아, 자체 데이터로 빠르고 가볍게 학습할 수 있는 FastText가 범용 모델인 BERT보다 효과적일 때가 많습니다.</p>
                    </aside>
                </article>
            </div>

            <div class="model-group highlight-orange fade-in-up">
                <h2>2. 동적 / 문맥적 임베딩 (Dynamic / Contextual Embedding)</h2>
                <p><b>문장 전체의 문맥을 실시간으로 파악하여, 같은 단어라도 상황에 맞는 다른 벡터값을 생성하는 방식입니다.</b> 진짜 언어처럼, 단어의 의미는 문장에 따라 결정된다는 아이디어를 구현했습니다.</p>

                <article class="sub-card">
                    <h4>작동 원리 (Contextualization)</h4>
                    <p>단어 하나가 아닌, 문장 전체가 모델에 입력됩니다. 모델은 어텐션(Attention) 메커니즘을 통해 단어 간의 관계를 파악하고, 각 단어에 가장 적합한 '문맥 맞춤형' 벡터를 생성합니다.</p>
                     <aside>
                        <p>문장 "맛있는 <b>사과</b>" 속 '사과' → [BERT] → <code>[0.15, -0.7, 0.8, ...]</code> (과일 의미)</p>
                        <p>문장 "진심으로 <b>사과</b>" 속 '사과' → [BERT] → <code>[-0.9, 0.5, 0.1, ...]</code> (용서 의미)</p>
                    </aside>
                </article>

                <article class="sub-card">
                    <h4>동적 임베딩의 장점 ✨</h4>
                    <aside>
                        <p><b>정적 임베딩 (사전) :</b> 'bank'를 찾으면 '강둑'과 '은행' 뜻이 모두 나열될 뿐, 구분해주지 않습니다.</p>
                        <p><b>동적 임베딩 (전문 통역사) :</b> "river bank"는 '강둑'으로, "savings bank"는 '은행'으로 알아서 구분하여 이해합니다.</p>
                    </aside>
                </article>

                <article class="sub-card">
                    <h4>워드 임베딩의 강력한 효과 🚀</h4>
                    <div>
                        <h5>1. 의미 관계 포착 🔗</h5>
                        <aside>
                            <p><b>유사성 :</b> '행복', '기쁨', '즐거움'은 서로 가까운 위치에 모여 있습니다.</p>
                            <p><b>관계 유추 :</b> <code>vector('서울') - vector('한국') + vector('일본') ≈ vector('도쿄')</code></p>
                        </aside>
                    </div>
                    <div>
                        <h5>2. 효율적인 차원 축소 📏</h5>
                        <aside>
                             <p><b>원-핫 인코딩(5천만 차원)</b>을 '주민등록증'처럼 <b>핵심 특징(300차원)</b>으로 압축합니다.</p>
                        </aside>
                    </div>
                    <div>
                        <h5>3. 모델 성능 향상 📈</h5>
                        <aside>
                            <p><b>더 높은 정확도 & 더 빠른 학습 :</b> 모델이 단어의 의미를 처음부터 알고 시작하므로('전이 학습' 효과) 더 적은 데이터와 시간으로도 좋은 성능에 도달할 수 있습니다.</p>
                        </aside>
                    </div>
                </article>
                
                <article class="sub-card">
                    <h4>어떤 TASK에서 주로 사용하는가? 🎯</h4>
                    <p>높은 정확도가 필요하고, 문맥을 미묘하게 이해해야 하는 거의 모든 현대 NLP Task에서 표준으로 사용됩니다.</p>
                    <aside>
                        <p><b>감성 분석 :</b> "이 배우 연기 미쳤다"에서 '미쳤다'가 극찬임을 파악합니다.</p>
                        <p><b>텍스트 분류 :</b> "애플 신제품 발표"에서 '애플'이 IT 기업임을 파악합니다.</p>
                        <p><b>개체명 인식 :</b> "김민준 씨가 부산으로 갔다"에서 '김민준'은 인명, '부산'은 지명임을 인식합니다.</p>
                    </aside>
                </article>
            </div>
        </div>

        <section class="section-title fade-in-up">
            <h2>3. 결론</h2>
        </section>
        <div class="model-group highlight-gray fade-in-up">
             <h2>단순한 숫자에서 '의미'를 지닌 좌표로</h2>
            <p>우리는 단어를 기계가 이해할 수 있도록 숫자로 바꾸는 여정을 살펴보았습니다.<br> 처음에는 각 단어에 단순히 번호표를 붙이는 <b>원-핫 인코딩</b>에서 시작했지만, 의미를 담지 못하는 명확한 한계가 있었습니다.</p>
            <p>그 다음, 단어의 '대표 의미'를 고정된 좌표로 표현하는 <b>정적 임베딩</b>이 등장하여 속도와 효율성 면에서 큰 발전을 이루었습니다.<br> 하지만 이 역시 '먹는 사과'와 '하는 사과'를 구분하지 못하는 한계를 보였습니다.</p>
            <p>마침내, 문장 전체를 보고 단어의 의미를 실시간으로 파악하는 <b>동적/문맥적 임베딩</b>이 등장하며 NLP 기술은 비약적인 발전을 이루었습니다.<br> 이 기술 덕분에 기계는 인간의 언어처럼 복잡하고 미묘한 문맥을 이해할 수 있게 되었습니다.</p>
            <p>이처럼 워드 임베딩의 발전은, 기계가 <b>단순한 기호(text)를 풍부한 의미(meaning)로</b> 받아들이게 된 과정이며,<br> 오늘날 우리가 경험하는 거의 모든 AI 언어 모델의 핵심 엔진이라고 할 수 있습니다.</p>
        </div>
    </main>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const animatedElements = document.querySelectorAll('.fade-in-up');

            if ('IntersectionObserver' in window) {
                const observer = new IntersectionObserver((entries) => {
                    entries.forEach(entry => {
                        if (entry.isIntersecting) {
                            entry.target.classList.add('visible');
                            observer.unobserve(entry.target);
                        }
                    });
                }, { threshold: 0.1 });

                animatedElements.forEach(el => {
                    observer.observe(el);
                });
            } else {
                animatedElements.forEach(el => {
                    el.classList.add('visible');
                });
            }
        });
    </script>

</body>
</html>
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RNN Explained: Final Layout</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@400;500;700&display=swap" rel="stylesheet">
    <style>
        body, html {
            margin: 0;
            padding: 0;
            font-family: 'Noto Sans KR', sans-serif;
            background-color: #ffffff; 
            color: #212529;
            line-height: 1.7;
        }

        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 3rem 2rem;
        }
        
        /* --- 카드 바깥의 메인 헤더 --- */
        .page-header {
            text-align: center;
            margin-bottom: 3rem;
        }

        .page-header h2 {
            font-size: 2.5rem;
            margin-bottom: 1rem;
        }
        
        .page-header p {
            font-size: 1.1rem;
            color: #495057;
            text-align: left; /* 첫 문단 왼쪽 정렬 */
        }


        /* --- 구분선 --- */
        hr.section-divider {
            border: none;
            border-top: 1px solid #dee2e6;
            margin: 4rem auto;
        }

        /* --- 대제목 (카드 바깥) --- */
        .section-title {
            text-align: center;
            margin-bottom: 2rem;
        }

        .section-title h2 {
            font-size: 2rem;
            font-weight: 700;
            margin: 0 0 1rem 0;
        }
        
        .section-title p { /* '학습 과정' 밑의 설명 */
             font-size: 1.1rem;
             color: #495057;
        }

        /* --- 카드 스타일 --- */
        .card {
            background-color: #f8f9fa; 
            border: 1px solid #e9ecef;
            border-radius: 12px;
            padding: 2rem;
            margin-bottom: 1.5rem;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 8px 25px rgba(0, 0, 0, 0.08);
        }

        .card h3 {
            font-size: 1.5rem;
            margin-top: 0;
            margin-bottom: 1rem;
            padding-bottom: 0.5rem;
            border-bottom: 1px solid #e9ecef;
        }
        
        .card h4 {
            font-size: 1.15rem;
            font-weight: 500;
            margin-top: 1.5rem;
            margin-bottom: 0.5rem;
            color: #343a40;
        }

        .card p {
            margin-top: 0;
            font-size: 1rem;
        }
        
        .card strong, .card b {
            font-weight: 700;
            color: #094074;
        }
        
        .card img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 2rem auto 1rem;
            border-radius: 8px;
            background-color: #ffffff;
            padding: 0.5rem;
            border: 1px solid #dee2e6;
        }

    </style>
</head>
<body>

<div class="container">
    
    <header class="page-header">
        <h2>RNN(Recurrent Neural Network)이란?</h2>
        <p>
        <b>순환 신경망(RNN, Recurrent Neural Network)</b>은 인공 신경망의 한 종류로, 내부에 <b>순환(Recurrent)</b>하는 구조를 사용하여 <b>순서가 있는 데이터(Sequential Data)</b>를 처리하는 데 특화된 모델입니다.
        <br>여기서 <b>'순환'</b>이란, 각 단계의 계산 결과가 단순히 출력으로 끝나는 것이 아니라, 다음 단계의 입력으로 다시 사용되는 <b>되먹임(loop)</b> 구조를 의미합니다.
        이 순환 구조를 통해 <b>'과거를 기억하는'</b> 아이디어가 구현됩니다. RNN은 <b>은닉 상태(Hidden State)</b>라는 내부적인 <b>'기억 장치'</b>를 가집니다.<br> 매 순간, 모델은 <b>'새로운 입력'</b>과 <b>'이전까지의 기억(은닉 상태)'</b>을 함께 받아 새로운 기억을 만듭니다.<br> 이 새로운 기억이 다음 순간으로 계속 전달되며 정보가 누적됩니다.
        </p>
    </header>
    <div class="card">
        <h3>RNN의 구조와 동작 원리 : 하나의 셀, 하나의 규칙</h3>
        <p>RNN의 모든 동작은 <strong>하나의 RNN 셀(Cell)</strong>과 <strong>하나의 규칙서(가중치)</strong>를 반복적으로 사용합니다.</p>
    </div>

    <div class="card">
        <h3>RNN 셀(Cell)의 구조</h3>
        <img src="../assets/RNN.png" alt="RNN Cell Unfolding Diagram">
        <p>하나의 RNN 셀은 특정 시점 t에서 다음과 같은 입출력 구조를 가집니다.</p>
        <p>
            <strong>Input Layer (입력)</strong><br>
            - 현재 정보 (xt) : 해당 시점의 새로운 데이터<br>
            - 과거 기억 (ht−1) : 바로 직전 시점까지의 정보를 요약한 은닉 상태
        </p>
        <p>
            <strong>Hidden Layer</strong><br>
            - 새로운 기억 (ht) : 현재 정보와 과거 기억을 종합하여 업데이트된 새로운 은닉 상태 (다음 시점으로 전달됨)
        </p>
        <p>
            <strong>Output Layer (출력)</strong><br>
            - 현재 결과 (ot) : 해당 시점에서 내놓는 예측값
        </p>
   </div>

    <div class="card">
        <h3>💡 최초의 기억은 어떻게?</h3>
        <p>첫 번째 RNN 셀에는 전달받을 과거 기억(h0)이 없습니다.<br> 따라서 이 값은 보통 모든 값이 <strong>‘0’인 '제로 벡터(Zero Vector)'나 아주 작은 랜덤 값</strong>으로<br> 직접 초기화해서 사용합니다.</p>

        <h3>💡 시퀀스 언롤링 (Sequence Unrolling)</h3>
        <p>실제로는 이 하나의 RNN 셀을 시퀀스의 길이만큼 복제하여<br> 시간 축에 따라 길게 펼쳐놓은 형태로 생각해야 합니다. 
        이를 <strong>'시퀀스 언롤링'</strong>이라고 합니다.<br>위 셀(Cell)의 구조 이미지에서 화살표 우측의 이미지가 <b>언롤링(Unrolling)된 RNN의 모습</b>입니다.
        <br> 언롤링은 각 시점별 정보의 흐름을 명확히 보여주며, 이후 설명할 학습 과정(BPTT)을 위해 필수적입니다.</p>
    </div>

    <hr class="section-divider">

    <div class="section-title">
        <h2>RNN의 학습 과정 : 순전파부터 역전파까지 ⚙️</h2>
        <p>RNN의 학습은 언롤링된 네트워크 위에서 <strong>순전파 → 손실 계산 → 역전파(BPTT)</strong> 의 과정을 거칩니다.</p> </div>

    <div class="card">
        <h3>1. 순전파 (Forward Pass)</h3>
        <p>입력 시퀀스를 모델에 흘려보내며 각 시점의 기억(은닉 상태)과 예측값(출력)을 차례대로 계산하는 단계입니다.</p>
        <p><strong>은닉 상태 계산</strong><br> : "<strong>현재 입력(xt)"</strong>과 "이전 은닉 상태(ht−1)"를 각각의 가중치와 곱한 뒤 합쳐,<br> 새로운 은닉 상태(ht)를 만듭니다.</p>
        <p><strong>출력 계산</strong><br> : 계산된 <strong>"은닉 상태(ht)"</strong>를 다시 가중치와 곱해 최종 예측값(yt)으로 변환합니다.</p>
    </div>
    
    <div class="card">
        <h3>2. 손실 계산 (Loss Calculation)</h3>
        <p>순전파를 통해 얻은 <b>예측값(yt)이 실제 정답과 얼마나 다른지(오차)를 계산</b>하는 단계입니다.<br>
        각 시점의 오차(ℓt)를 모두 합산하여 전체 시퀀스에 대한 <strong>총손실(L)</strong>을 구합니다.<br> 이 <strong>총손실(L)</strong>이 바로 다음에 이어질 역전파 단계의 출발점이 됩니다.</p>
    </div>

    <div class="card">
        <h3>3. BPTT (Backpropagation Through Time)</h3>
        <p>계산된 <strong>총손실(L)</strong>을 마지막 시점부터 첫 시점까지 시간의 역순으로 전파하며,<br> 모든 시점에서 <strong>공유되는 가중치(Wxh,Whh,Why)</strong>를 얼마나 수정해야 할지 기울기를 계산하는 알고리즘입니다.<br> 
        BPTT를 통해 RNN은 전체 시퀀스의 시간적 의존성을 반영하여 학습할 수 있습니다.<br> 
        학습의 마지막 단계에서는 BPTT로 모은 기울기 정보를 <strong>옵티마이저(Optimizer, 예: Adam, SGD)</strong>에 전달하여 최종적으로 가중치를 업데이트합니다.<br>이 전체 과정(순전파~업데이트)을 여러 번 반복하며 모델의 성능을 점차 개선시킵니다.</p>
    </div>

    <hr class="section-divider">

    <div class="section-title">
        <h2>RNN의 한계와 극복 방안</h2>
        <p>기본적인 RNN은 구조적 한계로 인해 몇 가지 중요한 문제점을 가집니다.</p>
    </div>

    <div class="card">
        <h3>1. 장기 의존성 문제 (Long-term Dependency)</h3>
        <p>가장 치명적인 문제로, 시퀀스가 길어질수록 <b>초반부의 중요한 정보가 뒤쪽까지 제대로 전달되지 못하고</b><br> 기억에서 사라지는 현상입니다. 이는 아래 설명할 기울기 문제 때문에 발생합니다.</p>
    </div>

    <div class="card">
        <h3>2. 기울기 소실 / 폭주 (Vanishing / Exploding Gradient)</h3>
        <p>BPTT 과정에서 동일한 가중치가 반복적으로 곱해지면서, <br><b>기울기가 0에 가깝게 사라지거나(소실) 무한대로 커져버리는(폭주) 현상</b>이 발생합니다.
        <br>이로 인해 학습이 멈추거나 불안정해집니다.</p>
    </div>

    <div class="card">
        <h3>3. 병렬 연산의 어려움</h3>
        <p>RNN은 이전 시점의 계산이 끝나야 다음 시점의 계산을 시작할 수 있는 순차적인 구조입니다.
        <br>이 때문에 여러 계산을 동시에 처리하는 병렬 연산이 어려워 학습 속도가 느릴 수 있습니다.</p>
    </div>

    <hr class="section-divider">

    <div class="section-title">
        <h2> 해결 방안</h2>
    </div>

    <div class="card">
        <h3>게이트 RNN (LSTM · GRU) ✨</h3>
        <p>정보의 흐름을 통제하는 <strong>'게이트(Gate)'</strong>를 내부에 두어, <br><b>중요한 정보는 오래 보존하고 불필요한 정보는 걸러내어</b><br>장기 의존성 문제를 해결하는 가장 근본적인 해결책입니다.</p>
    </div>

    <div class="card">
        <h3>기울기 클리핑 (Gradient Clipping)</h3>
        <p>기울기가 일정 임계값을 넘으면 강제로 잘라내어 기울기 폭주를 막는 기술적 방법입니다.</p>
    </div>

    <div class="card">
        <h3>Truncated BPTT</h3>
        <p>역전파의 길이를 일정 수준으로 제한하여 너무 먼 과거까지 기울기가 전파되지 않도록 하는 방법입니다.</p>
    </div>

    <hr class="section-divider">

<div class="section-title">
        <h2>RNN의 입출력 구조</h2>
    </div>

    <div class="card">
        <h3>One-to-One & One-to-Many</h3>
        
        <h4>1. One-to-One (비순차적 처리)</h4>
        <p>하나의 입력을 받아 하나의 출력을 내는 가장 기본적인 신경망 구조입니다.<br> RNN의 핵심인 <b>'순환'</b> 기능을 사용하지 않기 때문에, <b>RNN의 대표적인 활용 사례로 보기는 어렵습니다.</b><br>
        <b>예시</b> : 공부 시간(입력 1개)으로 시험 점수(출력 1개) 예측하기</p>
        
        <hr style="border-top: 1px solid #e9ecef; margin: 2rem 0;">

        <h4>2. One-to-Many (생성 모델)</h4>
        <p>하나의 '씨앗' 같은 입력을 받아 <b>여러 개의 연속된 데이터(시퀀스)</b>를 출력합니다.<br> 주로 <b>'생성'</b> 작업에 쓰입니다.</p>
        <p><b>- 이미지 캡셔닝</b><br>: 이미지의 특징 벡터(One)를 입력받아, 이미지를 설명하는 문장(Many)을 생성합니다.<br>
        <b>- 텍스트 생성</b><br>: 문장의 주제(One)를 입력받아, 그에 맞는 글(Many)을 자동으로 작문합니다.<br>
        <b>- 음악 생성</b><br>: 음악 장르나 첫 음(One)을 입력받아, 어울리는 멜로디(Many)를 생성합니다.</p>
    </div>

    <div class="card">
        <h3>Many-to-One</h3>
        <p><b>입력 N개 (시퀀스) → 출력 1개</b></p>
        <p>순차적인 데이터를 모두 입력받은 후, 마지막에 단 하나의 결론을 도출합니다.</p>
        
        <h4>주요 활용 Task</h4>
        <p><b>감성 분석 (Sentiment Analysis)</b><br>: 영화 리뷰 문장(단어 시퀀스) 전체를 읽고, 최종적으로 '긍정' 또는 '부정'이라는 단 하나의 라벨을 출력합니다.<br>
        <b>문서 분류</b><br>: 뉴스 기사 전체를 읽고 '스포츠', '정치' 등 카테고리 하나를 결정합니다.</p>
        
        <h4>코드 관점의 차이점</h4>
        <p>RNN이 “<b>문장 전체”</b>를 끝까지 다 읽도록 순전파를 진행합니다. 중간 단계의 출력들은 모두 무시하고,<br> 오직 <b>마지막 시점의 은닉 상태(final hidden state)</b>만을 사용합니다. <br>이 '최종 요약 벡터'를 분류기(nn.Linear)에 넣어 단 하나의 결과를 얻습니다.</p>
    </div>

    <div class="card">
        <h3>Many-to-Many 📜➡️📜</h3>
        <p><b>입력 N개 (시퀀스) → 출력 M개 (시퀀스)</b></p>
        <p>이 구조는 입력과 출력 시퀀스의 관계에 따라 두 가지로 나뉩니다.</p>

        <h4>1. 동기식 (Synced) Many-to-Many</h4>
        <p>입력 시퀀스의 길이와 출력 시퀀스의 길이가 같고, <b>각 시점의 입력이 각 시점의 출력과 1:1로 대응됩니다.</b></p>
        <p><b>주요 활용 Task</b><br>
        <b>품사 태깅 (Part-of-Speech, POS Tagging)</b><br>: "I love you" (3개 단어)를 입력받아, 각 단어에 해당하는 "대명사, 동사, 대명사" (3개 품사)를 출력합니다.<br>
        <b>영상 프레임별 분류</b><br>: 영상의 각 프레임(입력 시퀀스)이 '광고'인지 '본방송'인지 등을 판단합니다.</p>
        <p><b>코드 관점의 차이점</b><br>
        RNN이 “<b>문장 전체”</b>를 읽습니다.<br> 마지막 은닉 상태만 쓰는 Many-to-One과 달리, <b>모든 시점의 출력(sequence of outputs)</b>을<br> 전부 사용합니다. 이 출력 시퀀스 전체를 분류기에 넣어 각 단어에 대한 예측값을 얻습니다.</p>
        
        <hr style="border-top: 1px solid #e9ecef; margin: 2rem 0;">

        <h4><b>2. 비동기식 (Delayed) Many-to-Many (Seq2Seq)</b>👍</h4>
        <p>Many-to-Many 구조에서 <b>비동기식</b>을 압도적으로 더 많이 사용합니다.<br> 입력 시퀀스를 모두 읽은 후에, 그 의미를 바탕으로 새로운 출력 시퀀스를 생성합니다.<br> 입력과 출력의 길이가 달라도 괜찮습니다.<br> 이 구조를 특별히 <b>Seq2Seq (Encoder-Decoder) 모델</b>이라고 부릅니다.</p>
        <p><b>주요 활용 Task</b><br>
        <b>기계 번역</b><br>: "안녕하세요" (1개 단어)를 입력받아, "Hello" (1개 단어) 또는 "How are you" (3개 단어)를 출력합니다.<br>
        <b>챗봇</b><br>: 사용자의 질문(입력 시퀀스)을 이해하고 답변(출력 시퀀스)을 생성합니다.</p>
        <p><b>코드 관점의 차이점</b><br>
        <b>인코더(Encoder) RNN</b><br>: 입력 문장 전체를 읽고, 그 의미를 하나의 <b>문맥 벡터(context vector)</b>로 압축합니다. (Many-to-One과 유사)<br>
        <b>디코더(Decoder) RNN</b><br>: 인코더가 만든 문맥 벡터를 자신의 첫 번째 은닉 상태 초기값으로 받습니다. 이후, 디코더는 이 문맥을 바탕으로 출력 시퀀스를 한 단어씩 생성합니다. (One-to-Many와 유사)</p>
    </div>

</div>
</body>
</html>
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LSTM의 등장 이유와 GRU와의 비교</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@400;700;900&display=swap" rel="stylesheet">
    <style>
        /* I. Core Philosophy & Aesthetics */
        :root {
            --primary-text: #212529;
            --secondary-text: #6c757d;
            --border-color: #dee2e6;
            --light-gray-border: #e9ecef;
            --background-light: #ffffff;
            --background-gray: #f8f9fa;
            /* Updated Gate Colors */
            --highlight-red-bg: #fde8e8;
            --highlight-red-border: #f9c6c6;
            --highlight-blue-bg: #eef2f9;
            --highlight-blue-border: #cdd8e9;
            --highlight-green-bg: #f4f9f4;
            --highlight-green-border: #d8e8d8;
            --highlight-teal-bg: #eef7f7;
            --highlight-teal-border: #c7e8e8;
            --highlight-yellow-bg: #fef6e4;
            --highlight-yellow-border: #f3d5a2;
            /* New Navy Color for Cell State */
            --highlight-navy-bg: #e6eaf0;
            --highlight-navy-border: #ccd3dd;
        }

        body {
            font-family: 'Noto Sans KR', sans-serif;
            background-color: var(--background-light);
            color: var(--primary-text);
            margin: 0;
            padding: 0;
            line-height: 1.8;
            font-size: 16px;
        }

        /* II. Layout & Structure */
        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 40px 20px;
        }

        .header {
            text-align: center;
            margin-top: 0;
            margin-bottom: 50px;
        }

        .section-title {
            text-align: center;
            margin-bottom: 25px;
            margin-top: 50px;
        }
        
        .section-divider {
            border: none;
            height: 1px;
            background-color: var(--border-color);
            margin: 80px auto 60px auto;
        }

        /* Card System */
        .conceptual-card {
            background-color: var(--background-gray);
            border: 1px solid var(--border-color);
            border-radius: 12px;
            padding: 30px;
            text-align: left;
            transition: box-shadow 0.3s ease-in-out, transform 0.3s ease-in-out;
            margin-top: 25px;
        }
        
        .conceptual-card:hover {
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.08);
            transform: translateY(-5px);
        }

        .sub-card {
            background-color: var(--background-light);
            border: 1px solid var(--light-gray-border);
            border-radius: 8px;
            padding: 20px;
            margin-top: 20px;
            margin-bottom: 20px;
        }
        
        .card-grid {
             display: grid;
             gap: 20px;
             grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
             margin-top: 20px;
        }

        .card-image {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 10px auto 20px auto;
            display: block;
        }

        .formula {
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;
            background-color: #e9ecef;
            padding: 1rem;
            border-radius: 8px;
            text-align: center;
            font-size: 1.1rem;
            line-height: 1.6;
        }

        /* Comparison Table */
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 1rem;
        }
        .comparison-table th, .comparison-table td {
            border: 1px solid var(--border-color);
            padding: 15px;
            text-align: left;
            vertical-align: top;
        }
        .comparison-table th {
            background-color: var(--light-gray-border);
        }
        .comparison-table tr:nth-child(even) {
            background-color: var(--background-gray);
        }
        .comparison-table td ul {
            padding-left: 20px;
            margin: 0;
        }
        
        .standalone-p {
            font-size: 1.1rem;
            color: var(--secondary-text);
            max-width: 842px; /* Card width(900) - padding(30*2) + border(1*2) = 842px */
            margin: 25px auto;
            text-align: center;
        }

        /* III. Typography & Content Formatting */
        h1 {
            font-size: 2.5rem;
            font-weight: 900;
            margin-bottom: 0.5rem;
        }
        h2 {
            font-size: 2rem;
            font-weight: 700;
            margin: 0 0 1rem 0;
        }
        .section-title h1.section-title-large {
            font-size: 2.25rem;
        }

        h3 {
            font-size: 1.5rem;
            font-weight: 700;
            margin-top: 0;
            margin-bottom: 1rem;
        }
        h4 {
            font-size: 1.2rem;
            font-weight: 700;
            margin-top: 1.5rem;
            margin-bottom: 1rem;
        }

        .header p, .section-title p {
            font-size: 1.1rem;
            color: var(--secondary-text);
            max-width: 700px;
            margin: 1rem auto;
        }
        
        p {
            margin-top: 0;
            margin-bottom: 1rem;
        }
        
        p:last-child {
            margin-bottom: 0;
        }

        strong, b {
            font-weight: 700;
            color: var(--primary-text);
        }
        
        /* Highlight Colors */
        .highlight-red { background-color: var(--highlight-red-bg); border-color: var(--highlight-red-border); }
        .highlight-blue { background-color: var(--highlight-blue-bg); border-color: var(--highlight-blue-border); }
        .highlight-green { background-color: var(--highlight-green-bg); border-color: var(--highlight-green-border); }
        .highlight-yellow { background-color: var(--highlight-yellow-bg); border-color: var(--highlight-yellow-border); }
        .highlight-teal { background-color: var(--highlight-teal-bg); border-color: var(--highlight-teal-border); }
        .highlight-navy { background-color: var(--highlight-navy-bg); border-color: var(--highlight-navy-border); }
        .highlight-cec { background-color: #f1f3f5; border-color: #e2e6eb; }


    </style>
</head>
<body>

    <div class="container">

        <header class="header">
            <h1>LSTM가 등장한 이유</h1>
            <p>RNN의 한계점에서부터 Gated RNN의 등장까지</p>
        </header>

        <div class="section-title">
            <h1>RNN의 한계점<br>: Exploding / Vanishing Gradient</h1>
        </div>
        <p class="standalone-p">RNN은 순서가 있는 데이터를 처리하기 위해 <strong>이전 시점의 정보(hidden state)를 다음 시점으로 계속 넘겨줍니다.<br> 이 과정에서 똑같은 가중치 행렬(W, W_hh)</strong>을 계속해서 곱하게 됩니다.</p>
    
        <section class="conceptual-card">
            <h2>반복되는 곱셈으로 인한 문제점</h2>
            <p>RNN은 <strong>BPTT(Backpropagation Through Time)</strong>라는 방식으로 학습하는데, 이 과정에서 기울기(Gradient)를 계산할 때, <b>똑같은 가중치 행렬 W가 시퀀스의 길이만큼 반복적으로 곱해집니다.</b><br>그 곱해지는 값이 1보다 크거나 작을 경우 문제가 발생하게 됩니다.</p>
            <div class="card-grid">
                <div class="sub-card highlight-red">
                    <h3>📈 곱해지는 값이 1보다 크다면?</h3>
                    <p style="font-weight: 700; color: var(--primary-text);">(Exploding Gradient)</p>
                    <p>예를 들어, 1.2를 계속 곱하면 (1.2 → 1.44 → 1.728...) 값이 기하급수적으로 커져서, <br><b>무한대(inf, ∞)로 발산합니다.</b></p>
                    <p>이처럼 기울기 값이 비정상적으로 커지면 가중치가 너무 크게 업데이트되어 <b>학습이 불안정해지고 모델이 수렴하지 못하는 문제</b>가 발생합니다.</p>
                </div>
                <div class="sub-card highlight-blue">
                    <h3>📉 곱해지는 값이 1보다 작다면?</h3>
                    <p style="font-weight: 700; color: var(--primary-text);">(Vanishing Gradient)</p>
                    <p>반대로 0.8을 계속 곱하면 (0.8 → 0.64 → 0.512...) 값이 <b>순식간에 0에 가까워집니다.</b></p>
                    <p>이것이 바로 <b>장기 의존성 문제</b>의 핵심 원인입니다.<br>기울기 값이 0으로 사라져 버리면, 시퀀스의 앞부분에 있는 중요한 정보가 뒤쪽까지 전달되지 못하고, 모델은 먼 과거의 정보를 학습할 수 없게 됩니다.</p>
                </div>
            </div>
            <div class="sub-card" style="margin-top: 30px;">
                <h3>결론</h3>
                <p><strong>"RNN은 구조적으로 동일한 가중치를 계속 곱하기 때문에, 시퀀스가 길어질수록 기울기가 폭발하거나(Exploding) 사라져서(Vanishing) 제대로 된 학습이 어렵다"</strong>라는 RNN의 고질적인 문제점을 지적하고 있는 것입니다.<br><br>그리고 바로 이 문제를 해결하기 위해, 정보를 무작정 곱하는 대신 <strong>'게이트(Gate)'</strong>라는 장치를 이용해 정보의 흐름을 조절하는 <b>LSTM과 GRU</b>가 등장하게 됩니다.</p>
            </div>
        </section>

        <hr class="section-divider">

        <header class="header">
            <h1>Gated RNN (LSTM, GRU)의 등장</h1>
        </header>
        <P><img src="../assets/RNN_LSTM_GRU.png" alt="LSTM Cell Architecture" class="card-image"></P>
        <p class="standalone-p">RNN은 시계열 데이터나 자연어와 같이 순서가 중요한 데이터를 처리하는 데 특화된 신경망이지만, <b>장기 의존성 문제(오래전 정보가 뒤로 갈수록 희미해지는 현상)</b>라는 한계가 있습니다. LSTM과 GRU는 바로 이 문제를 해결하기 위해 등장한 모델입니다.</p>
        
        <div class="section-title">
            <h2>LSTM(Long Short-Term Memory, 장단기 기억)</h2>
        </div>
        <section class="conceptual-card">
            <p>LSTM은 <b>'장기 의존성 문제'</b>를 해결하기 위해 <strong>'셀 상태(Cell State)'</strong>라는 중요한 개념을 도입했습니다.<br>이 셀 상태가 효과적으로 작동하는 핵심 원리를 바로 <strong>CEC(Constant Error Carousel)</strong>라고 부릅니다.<br><br>셀 상태는 정보가 전체 시퀀스를 따라 큰 변화 없이 흐르도록 하는 일종의 컨베이어 벨트 역할을 합니다.<br>그리고 이 컨베이어 벨트에 정보를 추가하거나 제거하는 역할을 하는 <strong>세 개의 문(Gate)</strong>이 존재합니다.</p>
        </section>

        <p class="standalone-p">LSTM의 핵심을 이해하기 위해서는, <br>먼저 '기억'을 다루는 두 가지 핵심 요소와 이 기억이 어떻게 관리되는지 알아야 합니다.</p>

        <div class="section-title">
             <h1 class="section-title-large">셀 상태(Cell State)와 은닉 상태(Hidden State)</h1>
        </div>
        <section class="conceptual-card">
            <div class="sub-card" style="background-color: #f1f3f5;">
                <h2>① 장기 기억과 단기 기억</h2>
                LSTM 셀 안에는 두 가지 종류의 '기억'이 있습니다.
                <div class="sub-card highlight-navy">
                    <h3>Cell State, Ct : 장기 기억</h3>
                    <p>이것이 바로 LSTM의 가장 중요한 혁신입니다.
                    <br>셀 상태는 정보가 전체 문장을 따라 거의 변하지 않고 쭉 흘러갈 수 있도록 하는 통로 역할을 합니다.
                    <br>문장의 주어나 <b>전체적인 맥락과 같이 정말 중요한 핵심 정보들이</b> 여기에 기록되고 오랫동안 보존됩니다.</p>
                </div>
                <div class="sub-card highlight-blue">
                    <h3>Hidden State, ht : 단기 기억</h3>
                    <p>Cell State의 방대한 정보와 현재 들어온 정보를 바탕으로,<br> 
                    <strong>'지금 당장 필요한 내용만 요약'</strong>처럼 단기적으로 필요한 정보와 같습니다.<br>
                    <br>이 요약본은 두 가지 중요한 역할을 합니다.</p>
                    <ol>
                        <li>해당 시점의 <strong>예측 결과(예: 다음 단어)</strong>를 <strong>출력(Output)</strong>하는 데 사용됩니다.</li>
                        <li>다음 시점의 셀로 전달되어, 다음 단어를 판단하는 데 필요한 단기적인 문맥을 제공합니다.</li>
                    </ol>
                </div>
            </div>
            <p class="standalone-p" style="max-width:100%; text-align:left; margin-top:30px; margin-bottom:30px;">두 종류의 기억이 있다는 것을 알았으니,<br>이제 더 중요한 <b>장기 기억(셀 상태)이 어떻게 똑똑하게 관리되는지</b> 살펴보겠습니다.</p>
        <div class="sub-card" style="background-color: #f1f3f5;">
            <h2>② 셀 상태는 어떻게 업데이트 되는가?</h2>
            <p>셀 상태는 단순히 게이트들의 정보를 섞는 것이 아니라,
            <br><b>'이전 기억을 토대로 지우고 새로 쓰는' 과정을 통해 만들어집니다.</b><br>이 과정은 두 단계로 이루어집니다.</p>
            <div class="sub-card highlight-red">
                <h4>1. 지우기 (Forget)</h4> 
                <p>먼저, 이전 셀 상태, C<sub>t−1</sub>를 가져옵니다.
                <br>그리고 <strong>망각 게이트(Forget Gate)</strong>을 통해 
                <strong>"덜 중요한 기억(정보)"</strong>라고 판단되는 부분을 지웁니다.</p>
            </div>
            <div class="sub-card highlight-blue">
                <h4>2. 더하기 (Input)</h4>
                <p>그 다음, <strong>입력 게이트(Input Gate)</strong>를 통해 
                <br><strong>"새로 들어온 정보가 중요하다"</strong>라고 판단되는 내용을 빈 공간에 새로 추가합니다.</p>
            </div>
        </div>
                    <div class="sub-card" style="border-style: dashed;">
                <h3>결론 : 선택적 기억 업데이트</h3>
                <p>결론적으로, <strong>현재의 셀 상태(C<sub>t</sub>)</strong>란 <strong>과거의 기억(C<sub>t−1</sub>)</strong>에서 잊을 부분은 잊고(Forget), 
                <br>새로운 정보를 선별하여 더한(Input) 최종 결과물인 것입니다. <br>
                <br>이처럼 필요한 정보는 남기고 불필요한 정보는 버리는 <b>'선택적 업데이트'</b> 과정이 
                <br>바로 LSTM이 장기적인 맥락을 기억할 수 있는 핵심 비결입니다.</p>
            </div>
        </section>

        <p class="standalone-p">그렇다면, 이 '지우고 더하는' 방식이 어떻게 RNN의 근본적인 문제였던 기울기 소실을 해결할 수 있었을까요?</p>

        <div class="section-title">
            <h2>셀 상태의 비밀 : Constant Error Carousel (CEC)</h2>
        </div>
        <section class="conceptual-card">
            <div class="sub-card highlight-cec">
                <p>셀 상태가 통로의 역할을 하면서 기울기 소실 문제를 해결할 수 있는 근본적인 이유, 
                <br>바로 <strong>Constant Error Carousel (CEC)</strong>라는 메커니즘 덕분입니다.</p>
                <p>앞서 RNN이 가중치를 계속 <strong>'곱해서'</strong> 기울기 소실 문제가 생긴다고 했죠?<br>
                CEC는 이 문제를 정면으로 해결합니다.<br>핵심은 셀 상태가 업데이트될 때 '곱셈'이 아닌 <strong>'덧셈'</strong>을 기본 연산으로 사용한다는 점입니다.</p>
                <p class="formula">C_t = (Forget_Gate * C_{t-1}) + (Input_Gate * New_Info)</p>
                <p>이 <b>덧셈</b> 구조 덕분에, <b>역전파 시 기울기(오차)가 곱셈으로 인해 급격히 작아지는 현상</b>을 막고<br>먼 과거까지 거의 그대로(<strong>Constant</strong>) 전달될 수 있습니다.
                <br><b>'Carousel'</b>은 정보가 회전목마처럼 사라지지 않고 계속 순환하며 흐를 수 있다는 것을 비유합니다.<br>이것이 LSTM이 장기 의존성을 학습할 수 있는 비결입니다.</p>
            </div>
        </section>

        <p class="standalone-p">이제 이 모든 과정을 관장하는 문지기, 즉 게이트들에 대해 자세히 알아보겠습니다.</p>

        <div class="section-title">
            <h2>LSTM Gate 종류</h2>
        </div>
        <section class="conceptual-card">
            <div class="card-grid">
                <div class="sub-card highlight-red">
                    <h3>1. 망각 게이트 (Forget Gate)</h3>
                    <p>과거의 어떤 정보를 잊어버릴지 결정합니다.<br>시그모이드 함수를 통해
                    <br>0(모두 잊기)과 1(모두 기억) 사이의 값을 출력하여 
                    <br>이전 셀 상태에서 버릴 정보를 정합니다.</p>
                </div>
                <div class="sub-card highlight-yellow">
                    <h3>2. 입력 게이트 (Input Gate)</h3>
                    <p>현재 입력된 정보 중 어떤 것을 셀 상태에 저장할지 결정합니다.
                    <br>시그모이드 함수로 어떤 값을 업데이트할지 정하고, tanh 함수로 새로운 후보 값 벡터를 만들어 셀 상태에 더할 정보를 생성합니다.</p>
                </div>
                 <div class="sub-card highlight-green">
                    <h3>3. 출력 게이트 (Output Gate)</h3>
                    <p>셀 상태를 바탕으로 어떤 값을 출력할지 결정합니다.
                    <br>시그모이드 함수로 셀 상태의 어느 부분을 출력할지 정하고, 셀 상태를 tanh 함수에 통과시킨 값과 곱하여 최종 출력을 만듭니다.</p>
                </div>
            </div>
        </section>

        <hr class="section-divider">

        <div class="section-title">
            <h2>GRU (Gated Recurrent Unit)</h2>
        </div>
        <p class="standalone-p">GRU는 LSTM의 복잡한 구조를 더 간단하게 만든, 효율적인 대안입니다.</p>
        <section class="conceptual-card">
             <p>GRU는 LSTM의 복잡한 구조를 더 간단하게 만든 모델입니다.
            <br>LSTM의 핵심 아이디어인 '게이트'는 유지하되, <b>셀 상태와 은닉 상태를 하나로 통합</b>하고, 
            <br><b>게이트도 세 개에서 두 개</b>로 줄였습니다.<br>구조가 단순해진 만큼 계산 효율성이 높고, 때로는 LSTM과 비슷한 성능을 보이기도 합니다.</p>
        </section>
        
        <p class="standalone-p">GRU의 핵심은 <b>'통합된 기억'</b>과 이를 관리하는 <b>'두 개의 게이트'</b>입니다.</p>

        <div class="section-title">
            <h2>GRU의 핵심</h2>
            <h2>1. 통합된 기억 : 은닉 상태 (Hidden State)</h2>
        </div>
        <section class="conceptual-card">
            <div class="sub-card">
               <p>GRU에는 LSTM처럼 분리된 <b>Cell State</b>는 없습니다.<br>대신 <b>Hidden State</b> 하나가 장기 기억과 단기 기억의 역할을 모두 수행합니다.</p>
            </div>
        </section>

        <div class="section-title">
            <h2>2. GRU Gate 종류</h2>
        </div>
        <section class="conceptual-card">
             <div class="card-grid">
                 <div class="sub-card highlight-teal">
                    <h3>1. 리셋 게이트 (Reset Gate)</h3>
                    <p>과거의 정보와 현재 정보를 어떻게 조합할지 결정합니다.
                    <br>이 게이트가 닫히면(0에 가까운 값), 과거의 정보를 무시하고 현재 정보에 더 집중하게 됩니다.<br>문장의 주제가 완전히 바뀌는 지점에서, 리셋 게이트는 이전 주제와 관련된 과거 정보를 초기화하는 역할을 합니다.</p>
                 </div>
                 <div class="sub-card highlight-blue">
                    <h3>2. 업데이트 게이트 (Update Gate)</h3>
                    <p>과거 정보를 얼마나 유지하고, 현재 정보를 얼마나 새로 추가할지 결정합니다.
                        <br>LSTM의 <b>망각 게이트와 입력 게이트를 합쳐놓은 것</b>과 유사한 역할을 합니다.
                        <br>업데이트 게이트는 <b>이전 문맥을 얼마나 기억할지와 새로운 정보를 얼마나 반영할지 그 비율을 조절</b>합니다.</p>
                 </div>
            </div>
        </section>

        <hr class="section-divider">

        <div class="section-title">
            <h2>LSTM vs. GRU 핵심 비교표</h2>
        </div>
        <section class="conceptual-card">
            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>구분</th>
                        <th>LSTM (Long Short-Term Memory)</th>
                        <th>GRU (Gated Recurrent Unit)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>아이디어</strong></td>
                        <td>정보의 통로 역할을 하는 <b>Cell State</b>를 도입하여 장기 기억을 효과적으로 보존</td>
                        <td>LSTM의 복잡한 구조를 단순화하여 <br><b>Hidden State</b>가 모든 역할을 수행</td>
                    </tr>
                    <tr>
                        <td><strong>State</strong></td>
                        <td>Cell state(장기 기억) + Hidden state(단기 기억) 구조</td>
                        <td>Hidden State 하나로 통합된 구조</td>
                    </tr>
                    <tr>
                        <td><strong>게이트<br>종류</strong></td>
                        <td>
                            <ul>
                                <li><strong>망각 게이트 (Forget Gate)</strong><br>: 과거 정보를 얼마나 잊을지 결정</li>
                                <li><strong>입력 게이트 (Input Gate)</strong><br>: 새로운 정보를 얼마나 기억할지 결정</li>
                                <li><strong>출력 게이트 (Output Gate)</strong><br>: 현재 시점의 출력을 어떻게 할지 결정</li>
                            </ul>
                        </td>
                        <td>
                            <ul>
                                <li><strong>리셋 게이트 (Reset Gate)</strong><br>: 과거 정보와 현재 정보의 조합 방식을 결정</li>
                                <li><strong>업데이트 게이트 (Update Gate)</strong><br>: 과거 정보와 새 정보의 반영 비율을 결정<br>(LSTM의 망각+입력 게이트 역할)</li>
                            </ul>
                        </td>
                    </tr>
                    <tr>
                        <td><strong>구조<br>복잡성</strong></td>
                        <td>더 복잡하고 파라미터 수가 많음</td>
                        <td>더 단순하고 파라미터 수가 적음</td>
                    </tr>
                    <tr>
                        <td><strong>계산<br>효율성</strong></td>
                        <td>상대적으로 계산량이 많고 느림</td>
                        <td>상대적으로 계산량이 적고 빠름</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <hr class="section-divider">

        <div class="section-title">
             <h2>언제 무엇을 사용해야 할까요?</h2>
        </div>
        <p class="standalone-p">어떤 시퀀스 모델이 더 좋은지에 대해 명확하게 밝혀진 것은 없습니다.<br>다만, LSTM과 GRU 모두 Vanilla RNN보다는 확실한 성능을 보장합니다.</p>

    </div>

</body>
</html>


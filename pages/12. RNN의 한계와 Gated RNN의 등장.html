<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LSTM의 등장 이유와 GRU와의 비교</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@400;700;900&display=swap');

        :root {
            --primary-text: #212529;
            --secondary-text: #6c757d;
            --border-color: #dee2e6;
            --background-light: #ffffff;
            --inset-bg: #f8f9fa;
            --highlight-orange-bg: #fff8f0;
            --highlight-orange-border: #ffe8cc;
            --highlight-gray-bg: #f8f9fa;
            --highlight-gray-border: #dee2e6;
            --highlight-red-bg: #fff5f5;
            --highlight-red-border: #ffd0d0;
            --highlight-blue-bg: #f0f7ff;
            --highlight-blue-border: #cce4ff;
            --highlight-green-bg: #f0fff4;
            --highlight-green-border: #cce8d0;
            --highlight-teal-bg: #eefbff;
            --highlight-teal-border: #c7eef5;
            --highlight-yellow-bg: #fffbeb;
            --highlight-yellow-border: #f5e8c7;
            --step-icon-bg: #2c3e50;
            --step-icon-text: #ffffff;
        }

        body {
            font-family: 'Noto Sans KR', sans-serif;
            background-color: var(--background-light);
            color: var(--primary-text);
            margin: 0;
            padding: 0;
            line-height: 1.8;
            font-size: 16px;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 40px 20px;
        }

        .header, .section-title {
            text-align: center;
            margin-bottom: 25px;
            margin-top: 50px;
        }
        
        .header {
            margin-top: 0;
            margin-bottom: 50px;
        }

        h1 {
            font-size: 2.5rem;
            font-weight: 900;
            margin-bottom: 0.5rem;
        }

        h2, .section-title h3 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
            display: flex;
            align-items: center;
            justify-content: center;
        }
        
        .section-title h3 {
            font-size: 1.5rem;
        }

        h3 {
            font-size: 1.5rem;
            font-weight: 700;
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
        }
        
        h4 {
            font-size: 1.2rem;
            font-weight: 700;
            margin-top: 2rem;
            margin-bottom: 1rem;
        }
        
        h5 {
            font-size: 1.1rem;
            font-weight: 700;
        }

        .header p, .section-title p, .standalone-p {
            font-size: 1.1rem;
            color: var(--secondary-text);
            max-width: 600px;
            margin: 1rem auto;
            text-align: center;
        }
        
        .full-width-p {
             max-width: 100%;
        }
        
        p {
            margin-top: 0;
            margin-bottom: 1rem;
        }

        strong, b {
            font-weight: 700;
            color: var(--primary-text);
        }

        .content-block, .step-card, .model-card {
            background-color: var(--background-light);
            border: 1px solid var(--border-color);
            border-radius: 12px;
            padding: 30px;
            text-align: left;
            transition: box-shadow 0.3s ease-in-out, transform 0.3s ease-in-out;
            opacity: 0;
            transform: translateY(20px);
            margin-top: 25px;
        }
        
        .content-block.visible, .step-card.visible, .model-card.visible {
            opacity: 1;
            transform: translateY(0);
        }

        .content-block:hover, .step-card:hover, .model-card:hover {
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.08);
            transform: translateY(-5px);
        }

        aside {
            background-color: var(--inset-bg);
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 20px;
            margin-top: 20px;
            margin-bottom: 20px;
        }
        
        aside aside {
             background-color: #fff;
             border: 1px dashed var(--border-color);
        }

        .step-number {
            display: inline-flex;
            align-items: center;
            justify-content: center;
            width: 40px;
            height: 40px;
            background-color: var(--step-icon-bg);
            color: var(--step-icon-text);
            font-size: 1.5rem;
            font-weight: 700;
            border-radius: 8px;
            margin-right: 15px;
            position: relative;
            text-shadow: 1px 1px 0px rgba(255,0,0,0.5), -1px -1px 0px rgba(0,255,255,0.5);
            flex-shrink: 0;
        }

        .steps-grid {
            display: grid;
            gap: 25px;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
        }
        
        .highlight-orange {
            background-color: var(--highlight-orange-bg);
            border-color: var(--highlight-orange-border);
        }
        .highlight-gray {
            background-color: var(--highlight-gray-bg);
            border-color: var(--highlight-gray-border);
        }
        .highlight-red-pale {
            background-color: var(--highlight-red-bg);
            border-color: var(--highlight-red-border);
        }
        .highlight-blue-pale {
            background-color: var(--highlight-blue-bg);
            border-color: var(--highlight-blue-border);
        }
        .highlight-green-pale {
            background-color: var(--highlight-green-bg);
            border-color: var(--highlight-green-border);
        }
         .highlight-teal-pale {
            background-color: var(--highlight-teal-bg);
            border-color: var(--highlight-teal-border);
        }
        .highlight-yellow-pale {
            background-color: var(--highlight-yellow-bg);
            border-color: var(--highlight-yellow-border);
        }

        .card-image {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 10px auto 20px auto;
            display: block;
        }
        
        .section-divider {
            border: none;
            height: 1px;
            background-color: var(--border-color);
            margin: 80px auto 60px auto;
        }

        .comparison-table-title {
            font-size: 1.8rem !important;
        }
        
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 2rem;
        }
        .comparison-table th, .comparison-table td {
            border: 1px solid var(--border-color);
            padding: 15px;
            text-align: left;
            vertical-align: top;
        }
        .comparison-table th {
            background-color: #e9ecef;
        }
        .comparison-table td ul {
            padding-left: 20px;
            margin: 0;
        }
    </style>
</head>
<body>

    <div class="container">

        <header class="header">
            <h1>LSTM가 등장한 이유</h1>
            <p>RNN의 한계점에서부터 Gated RNN의 등장까지</p>
        </header>

        <div class="section-title">
            <h2><span class="step-number">1</span>RNN의 한계점 : Exploding / Vanishing Gradient</h2>
        </div>
        <section class="content-block">
            <p>RNN은 순서가 있는 데이터를 처리하기 위해 <strong>이전 시점의 정보(hidden state)를 다음 시점으로 계속 넘겨줍니다.<br> 이 과정에서 똑같은 가중치 행렬(W,W_hh)</strong>을 계속해서 곱하게 됩니다.</p>
            <h4>그러면 왜 문제가 생길까? - 반복되는 곱셈</h4>
            <p>RNN의 학습은 <strong>BPTT(Backpropagation Through Time)</strong>라는 방식을 사용하는데,<br> 이 과정에서 기울기(Gradient)를 계산할 때, <b>똑같은 가중치 행렬 W가 시퀀스의 길이만큼 반복적으로 곱해집니다.</b></p>
        </section>

        <div class="section-title">
             <h2>Q. 똑같은 가중치가 곱해진다는 것은 무슨 뜻일까?</h2>
        </div>
        <section class="content-block">
            <aside>
                <h3>사람이 문장을 읽는 방식 (feat. RNN)</h3>
                <p>"나는 어제 학교에 갔다" 라는 문장을 읽는다고 상상해 봅시다.</p>
                <aside>
                    <h4>"나는"을 읽습니다.</h4>
                    <p>뇌는 <strong>'나'</strong>라는 단어를 보고 '아, 주어가 나왔구나'라고 판단합니다.<br>
                    (이때 뇌가 사용한 판단 규칙을 <strong>W</strong>라고 해볼게요.)<br>
                    머릿속에는 <strong>'주어’ = ‘나'</strong>라는 정보가 저장됩니다. (이것이 Hidden State 입니다)</p>
                </aside>
                <aside>
                    <h4><strong>“어제”</strong>를 읽습니다.</h4>
                    <p>뇌는 <strong>'어제'</strong>라는 단어를 보고 '시간 정보구나'라고 판단합니다.<br>
                    이때, '어제'를 판단하기 위해 뇌가 갑자기 새로운 규칙을 만들지 않아요.<br>
                    '나는'을 읽을 때 썼던 것과 <strong>똑같은 언어 처리 규칙(W)</strong>을 사용합니다.<br>
                    그리고 이 <strong>규칙(W)</strong>을 통해, <strong>'어제'</strong>라는 새 정보와 머릿속에 있던 <strong>'주어’ = ‘나'</strong>라는 기존 정보를 조합해서<br> '아, 내가 어제 한 일이구나'라고 이해를 업데이트합니다.</p>
                </aside>
                <aside>
                    <h4>"학교에"를 읽습니다.</h4>
                    <p>마찬가지로 <strong>'학교에'</strong>라는 단어를 보고 '장소 정보구나'라고 판단할 때도,<br> 이전에 썼던 것과 <strong>완전히 똑같은 그 규칙(W)</strong>을 또 사용합니다.<br>
                    이 규칙(W)으로 <strong>'학교에'</strong>라는 새 정보와 <strong>'내가 어제 한 일'</strong>이라는 기존 정보를 조합해서 이해를 발전시킵니다.</p>
                </aside>
                <p>이 과정처럼, RNN은 <strong>하나의 잘 만들어진 '문맥 파악 규칙'(가중치 W)</strong>을 가지고,<br> 이 규칙을 <br><b>첫 번째 단어</b>에도 적용하고,<br><b>두 번째 단어</b>에도 똑같이 적용하고,<br> <b>세 번째 단어</b>에도 똑같이 적용하면서 정보를 계속 업데이트해 나가는 것입니다.</p>
            </aside>
        </section>
        
        <div class="steps-grid">
            <div class="step-card highlight-red-pale">
                 <h3>💣 곱해지는 값이 1보다 크다면?</h3>
                 <p style="font-weight: 700; color: var(--primary-text);">(Exploding Gradient)</p>
                 <aside>
                    예를 들어, 1.2를 계속 곱하면<br> (1.2 → 1.44 → 1.728...) 값이 기하급수적으로<br> 커져서 <b>무한대(inf, ∞)로 발산합니다.</b>
                    <p>이처럼 기울기 값이 비정상적으로 커지면<br> 가중치가 너무 크게 업데이트되어 <br><b>학습이 불안정해지고 모델이 수렴하지 못하는 문제</b>가 발생합니다.</p>
                </aside>
            </div>
             <div class="step-card highlight-blue-pale">
                 <h3>📉 곱해지는 값이 1보다 작다면?</h3>
                 <p style="font-weight: 700; color: var(--primary-text);">(Vanishing Gradient)</p>
                 <aside>
                    반대로 0.8을 계속 곱하면<br> (0.8 → 0.64 → 0.512...) 값이<br> <b>순식간에 0에 가까워집니다.</b>
                    <p>이것이 바로 <b>장기 의존성 문제</b>의 핵심 원인입니다. 기울기 값이 0으로 사라져 버리면,<br> 시퀀스의 앞부분에 있는 중요한 정보가 뒤쪽까지 전달되지 못하고, 모델은 먼 과거의 정보를 학습할 수 없게 됩니다.</p>
                </aside>
            </div>
        </div>
        
        <div class="section-title">
             <h2><span class="step-number">3</span>요약</h2>
        </div>
        <p class="standalone-p">결론적으로, <strong>"RNN은 구조적으로 동일한 가중치를 계속 곱하기 때문에, 
         시퀀스가 길어질수록 기울기가 폭발하거나(Exploding) 사라져서(Vanishing) 제대로 된 학습이 어렵다"</strong>라는
         RNN의 고질적인 문제점을 지적하고 있는 것입니다.<br><br>그리고 바로 이 문제를 해결하기 위해,<br> 정보를 무작정 곱하는 대신 <strong>'게이트(Gate)'</strong>라는 장치를 이용해 정보의 흐름을 조절하는LSTM과 GRU가 등장하게 된 것이죠. 😊</p>

        <hr class="section-divider">

        <div class="section-title">
            <h1>🧠 Gated RNN (LSTM, GRU)의 등장</h1>
        </div>
        <p class="standalone-p full-width-p">RNN은 시계열 데이터나 자연어와 같이 순서가 중요한 데이터를 처리하는 데 특화된 신경망이지만,
        <b>장기 의존성 문제(오래전 정보가 뒤로 갈수록 희미해지는 현상)</b>라는 한계가 있었죠. LSTM과 GRU는 바로 이 문제를 해결하기 위해 등장한 모델입니다.</p>
        
        <div class="section-title">
            <h2>1. LSTM(Long Short-Term Memory, 장단기 기억) ✨</h2>
        </div>
        <section class="model-card highlight-orange">
            <img src="../assets/LSTM.png" alt="LSTM Cell Architecture" class="card-image">
            <p>LSTM은 <b>'장기 의존성 문제'</b>를 해결하기 위해 <strong>'셀 상태(Cell State)'</strong>라는 중요한 개념을 도입했습니다.<br>
            이 셀 상태가 효과적으로 작동하는 핵심 원리를 바로 <strong>CEC(Constant Error Carousel)</strong>라고 부릅니다.<br> 셀 상태는 정보가 전체 시퀀스를 따라 큰 변화 없이 흐르도록 하는 일종의 컨베이어 벨트 역할을 합니다. 그리고 이 컨베이어 벨트에 정보를 추가하거나 제거하는 역할을 하는 <strong>세 개의 문(Gate)</strong>이 존재합니다.</p>
        </section>

        <div class="section-title">
             <h2>셀 상태(Cell State)와 은닉 상태(Hidden State)</h2>
        </div>
        <section class="model-card highlight-orange">
            <h3>두 종류의 기억 : 장기 기억과 단기 기억</h3>
            <p>LSTM 셀 안에는 두 가지 종류의 '기억'이 있습니다.</p>
            <aside>
                <h3><p><strong>✅ 셀 상태 (Cell State, Ct) : 메모장의 본문 (장기 기억)</strong></p></h3>
                이것이 바로 LSTM의 가장 중요한 혁신입니다.<br> 
                셀 상태는 정보가 전체 문장을 따라 거의 변하지 않고 쭉 흘러갈 수 있도록 하는'정보 고속도로' 역할을 합니다.<br> 
                마치 메모장의 <strong>'본문’</strong>처럼, 문장의 주어나 전체적인 맥락과 같이 정말 중요한 핵심 정보들이 여기에 기록되고 오랫동안 보존됩니다.
            </aside>
            <aside>
                <h3><p><strong>✅ 은닉 상태 (Hidden State, ht) : 포스트잇 요약본 (단기 기억)</strong></p></h3>
                메모장 본문(셀 상태)의 방대한 정보와 현재 들어온 정보를 바탕으로, <strong>'지금 당장 필요한 내용만 요약'</strong>한 포스트잇과 같습니다.
                이 요약본은 두 가지 중요한 역할을 합니다.
                <p>해당 시점의 <strong>예측 결과(예: 다음 단어)</strong>를 <strong>출력(Output)</strong>하는 데 사용됩니다.<br>
                다음 시점의 셀로 전달되어, 다음 단어를 판단하는 데 필요한 단기적인 문맥을 제공합니다.</p>
            </aside>

            <h3>메모장 관리법 : 셀 상태는 어떻게 업데이트 되는가?</h3>
            <p>셀 상태는 단순히 게이트들의 정보를 섞는 것이 아니라,<br>
            <b>'이전 메모를 바탕으로 지우고 새로 쓰는' 과정을 통해 만들어집니다.</b> 이 과정은 두 단계로 이루어집니다.</p>
             <aside>
                <h4>1. 지우기 (Forget) 🗑️</h4> 먼저, 어제까지 쓴 메모(이전 셀 상태, Ct−1)를 가져옵니다.<br> 그리고 <strong>망각 게이트(Forget Gate)</strong>라는 지우개를 사용해 <strong>"이 정보는 이제 덜 중요하니 지우자"</strong>라고 판단되는 부분을 지웁니다.
                <h4>2. 더하기 (Input) ✍️</h4>그 다음, <strong>입력 게이트(Input Gate)</strong>라는 펜을 사용해<br> <strong>"이건 새로 들어온 중요한 정보니까 추가로 적자"</strong>라고 판단되는 내용을 빈 공간에 새로 써넣습니다.
            </aside>
            <p>결론적으로, <strong>현재의 셀 상태(Ct)</strong>란 <strong>과거의 기억(Ct−1)</strong>에서 잊을 부분은 잊고(Forget),<br>새로운 정보를 선별하여 더한(Input) 최종 결과물인 것입니다.</p>
        </section>

        <div class="section-title">
            <h2>셀 상태의 비밀 : Constant Error Carousel (CEC)</h2>
        </div>
        <section class="model-card highlight-orange">
            <aside>
                <p>셀 상태가 '정보 고속도로'처럼 작동하며 기울기 소실 문제를 해결할 수 있는 근본적인 이유<br> 바로 <strong>Constant Error Carousel (CEC)</strong>라는 메커니즘 덕분입니다.</p>
                <p>앞서 RNN이 가중치를 계속 <strong>'곱해서'</strong> 기울기 소실 문제가 생긴다고 했죠? 
                <br>CEC는 이 문제를 정면으로 해결합니다.<br>핵심은 셀 상태가 업데이트될 때 '곱셈'이 아닌 <strong>'덧셈'</strong>을 기본 연산으로 사용한다는 점입니다.</p>
                <p><code>Ct = (Forget Gate * Ct-1) + (Input Gate * New Info)</code></p>
                <p>이 <b>덧셈</b> 구조 덕분에, 역전파 시 기울기(오차)가 곱셈으로 인해 급격히 작아지는 현상을 막고<br> 
                먼 과거까지 거의 그대로(<strong>Constant</strong>) 전달될 수 있습니다. 
                <br><b>Carousel</b>'은 정보가 회전목마처럼 사라지지 않고 계속 순환하며 흐를 수 있다는 것을 비유합니다. 
                <br>이것이 LSTM이 장기 의존성을 학습할 수 있는 비결입니다.</p>
            </aside>
        </section>

        <div class="section-title">
            <h2>LSTM Gate 종류</h2>
        </div>
        <div class="steps-grid">
            <div class="model-card highlight-red-pale">
                <h3>1. 망각 게이트 (Forget Gate)</h3>
                 <aside>
                    과거의 어떤 정보를 잊어버릴지 결정합니다.<br>시그모이드 함수를 사용하여 0(모두 잊기)과 1(모두 기억) 사이의 값을 출력하여<br> 
                    이전 셀 상태에서 버릴 정보를 정합니다.
                </aside>
            </div>
            <div class="model-card highlight-yellow-pale">
                <h3>2. 입력 게이트 (Input Gate)</h3>
                <aside>
                    현재 입력된 정보 중 어떤 것을 셀 상태에 저장할지 결정합니다.<br>시그모이드 함수로 어떤 값을 업데이트할지 정하고, 하이퍼볼릭 탄젠트(tanh) 함수로 새로운 후보 값 벡터를 만들어 셀 상태에 더할 정보를 생성합니다.
                </aside>
            </div>
             <div class="model-card highlight-green-pale">
                <h3>3. 출력 게이트 (Output Gate)</h3>
                <aside>
                    셀 상태를 바탕으로 어떤 값을 출력할지 결정합니다. 시그모이드 함수로 셀 상태의 어느 부분을 출력할지 정하고,셀 상태를 tanh 함수에 통과시킨 값과 곱하여 최종 출력을 만듭니다.
                </aside>
            </div>
        </div>

        <hr class="section-divider">

        <div class="section-title">
            <h2>2. GRU (Gated Recurrent Unit)</h2>
        </div>
        <section class="model-card">
             <img src="../assets/GRU.png" alt="GRU Cell Architecture" class="card-image">
             <p>GRU는 LSTM의 복잡한 구조를 더 간단하게 만든 모델입니다.<br>
             LSTM의 셀 상태와 은닉 상태(Hidden State)를 하나로 통합하고, 게이트도 두 개로 줄였습니다.<br> 
             구조가 단순해진 만큼 계산 효율성이 높고, 때로는 LSTM과 비슷한 성능을 보이기도 합니다.</p>
        </section>

        <div class="section-title">
            <h2>GRU Gate 종류</h2>
        </div>
        <div class="steps-grid">
             <div class="model-card highlight-teal-pale">
                <h3>1. 리셋 게이트 (Reset Gate)</h3>
                <aside>
                    과거의 정보와 현재 정보를 어떻게 조합할지 결정합니다.<br> 
                    이 게이트가 닫히면(0에 가까운 값), 과거의 정보를 무시하고 현재 정보에 더 집중하게 됩니다.<br>
                    문장의 주제가 완전히 바뀌는 지점에서,리셋 게이트는 이전 주제와 관련된 과거 정보를 초기화하는 역할을 합니다.
                </aside>
             </div>
             <div class="model-card highlight-blue-pale">
                <h3>2. 업데이트 게이트 (Update Gate)</h3>
                <aside>
                    과거 정보를 얼마나 유지하고, 현재 정보를 얼마나 새로 추가할지 결정합니다.<br> 
                    LSTM의 망각 게이트와 입력 게이트를 합쳐놓은 것과 유사한 역할을 합니다.<br> 
                    업데이트 게이트는 이전 문맥을 얼마나 기억할지(예: 이야기의 주인공)와 새로운 정보(예: 주인공의 새로운 행동)를<br> 
                    얼마나 반영할지 그 비율을 조절합니다.
                </aside>
             </div>
        </div>
        
        <div class="section-title">
            <h2>GRU의 핵심 - 통합된 상태</h2>
        </div>
        <section class="model-card">
            <aside>
               GRU에는 LSTM처럼 분리된 <b>셀 상태(Cell State)</b>는 없습니다.<br>
               대신 은닉 상태(Hidden State) 하나가 그 두 가지 역할을 모두 수행합니다.<br>
               LSTM의 핵심이던 셀 상태와 은닉 상태를 <b>하나의 은닉 상태(Hidden State)</b>로 통합하였습니다.
            </aside>
        </section>

        <hr class="section-divider">

        <div class="section-title">
            <h1 class="comparison-table-title">LSTM vs. GRU 핵심 비교표</h1>
        </div>
        <section class="content-block highlight-gray">
            <table class="comparison-table">
                <thead>
                    <tr>
                        <th>구분</th>
                        <th>LSTM (Long Short-Term Memory)</th>
                        <th>GRU (Gated Recurrent Unit)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>아이디어</strong></td>
                        <td>'정보 고속도로' 역할을 하는 별도의 셀 상태(Cell State)를 도입하여 장기 기억을 효과적으로 보존</td>
                        <td>LSTM의 복잡한 구조를 단순화하여 하나의 상태(Hidden State)가 모든 역할을 수행</td>
                    </tr>
                    <tr>
                        <td><strong>상태 (기억)</strong></td>
                        <td>셀 상태 (장기 기억) + 은닉 상태 (단기 기억)로<br> 2원화된 구조</td>
                        <td>은닉 상태 (Hidden State) 하나로 통합된 구조</td>
                    </tr>
                    <tr>
                        <td><strong>게이트 종류</strong></td>
                        <td>
                            <ul>
                                <li><strong>망각 게이트 (Forget Gate)</strong> :<br> 과거 정보를 얼마나 잊을지 결정</li>
                                <li><strong>입력 게이트 (Input Gate)</strong> :<br> 새로운 정보를 얼마나 기억할지 결정</li>
                                <li><strong>출력 게이트 (Output Gate)</strong> :<br> 현재 시점의 출력을 어떻게 할지 결정</li>
                            </ul>
                        </td>
                        <td>
                            <ul>
                                <li><strong>리셋 게이트 (Reset Gate)</strong> :<br> 과거 정보와 현재 정보의 조합 방식을 결정</li>
                                <li><strong>업데이트 게이트 (Update Gate)</strong> :<br> 과거 정보와 새 정보의 반영 비율을 결정 (LSTM의 망각+입력 게이트 역할)</li>
                            </ul>
                        </td>
                    </tr>
                    <tr>
                        <td><strong>구조 복잡성</strong></td>
                        <td>더 복잡하고 파라미터 수가 많음</td>
                        <td>더 단순하고 파라미터 수가 적음</td>
                    </tr>
                    <tr>
                        <td><strong>계산 효율성</strong></td>
                        <td>상대적으로 계산량이 많고 느림</td>
                        <td>상대적으로 계산량이 적고 빠름</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <hr class="section-divider">

        <div class="section-title">
             <h2>언제 무엇을 사용해야 할까요? 🤔</h2>
        </div>
        <p class="standalone-p">어떤 시퀀스 모델이 더 좋은지에 대해 명확하게 밝혀진 것은 없습니다.<br> 다만, LSTM과 GRU 모두 Vanilla RNN보다는 확실한 성능을 보장합니다.</p>

    </div>

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const animatedElements = document.querySelectorAll('.content-block, .step-card, .model-card');

            const observer = new IntersectionObserver((entries) => {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        entry.target.classList.add('visible');
                        observer.unobserve(entry.target);
                    }
                });
            }, {
                threshold: 0.1
            });

            animatedElements.forEach(el => {
                observer.observe(el);
            });
        });
    </script>

</body>
</html>
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>토큰화(Tokenization) 심층 분석</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@400;700;900&display=swap" rel="stylesheet">
    <style>
        /* 1. Core Design Philosophy & 2. Technical Specifications */
        :root {
            --color-text-primary: #212529;
            --color-text-secondary: #6c757d;
            --color-border: #dee2e6;
            --color-bg-white: #ffffff;
            --color-bg-subtle: #f8f9fa;
            --color-step-icon-bg: #2c3e50;
            --font-family-base: 'Noto Sans KR', sans-serif;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: var(--font-family-base);
            font-size: 1rem;
            line-height: 1.7;
            background-color: var(--color-bg-white);
            color: var(--color-text-primary);
            margin: 0;
            padding: 4rem 1rem;
            -webkit-font-smoothing: antialiased;
        }

        /* 2. Typography & Hierarchy */
        h1, h2, h3 {
            font-weight: 700;
            line-height: 1.3;
            margin-top: 0;
        }

        h1 {
            font-size: 2.5rem;
            font-weight: 900;
            margin-bottom: 0.5rem;
        }

        h2 {
            font-size: 2rem;
            margin-bottom: 0.75rem;
        }

        h3 {
            font-size: 1.5rem;
            margin-bottom: 1rem;
        }
        
        h4 {
            font-size: 1.1rem;
            margin-bottom: 0.5rem;
            color: var(--color-step-icon-bg);
        }

        p {
            margin-top: 0;
            margin-bottom: 1rem;
        }
        
        strong, b {
           font-weight: 700;
           color: var(--color-text-primary);
        }
        
        code {
            font-family: Consolas, 'Courier New', monospace;
            background-color: #e9ecef;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-size: 90%;
        }

        /* 3. Component Library & Structure */
        .container {
            max-width: 800px;
            margin: 0 auto;
        }

        .header, .section-title {
            text-align: center;
            margin: 4rem auto 3rem auto;
        }
        
        .header {
            margin-top: 0;
        }
        
        .section-title p {
            font-size: 1.125rem;
            color: var(--color-text-secondary);
            max-width: 600px;
            margin-left: auto;
            margin-right: auto;
            margin-bottom: 0;
        }

        .card {
            background-color: var(--color-bg-subtle);
            border: 1px solid var(--color-border);
            border-radius: 0.75rem;
            padding: 2.5rem;
            margin-bottom: 1.5rem;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.02);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 20px rgba(0, 0, 0, 0.07);
        }

        .steps-grid {
            display: grid;
            grid-template-columns: 1fr; /* Stack cards vertically */
            gap: 1.5rem;
        }

        .step-card {
            display: flex;
            flex-direction: column;
            gap: 1.5rem;
        }
        
        @media (min-width: 768px) {
            .step-card {
                flex-direction: row;
                align-items: flex-start;
            }
        }
        
        .step-card-content {
            flex: 1;
        }
        
        .step-number {
            flex-shrink: 0;
            width: 50px;
            height: 50px;
            background-color: var(--color-step-icon-bg);
            color: #fff;
            border-radius: 8px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.75rem;
            font-weight: 700;
            text-shadow: 1px 1px 0px rgba(255, 0, 85, 0.5), -1px -1px 0px rgba(0, 229, 255, 0.5);
        }
        
        /* === [추가] 알고리즘 카드 컨테이너 === */
        .algo-card-container {
            display: grid;
            grid-template-columns: 1fr;
            gap: 1rem;
            margin-top: 1.5rem;
        }
        @media (min-width: 768px) {
            .algo-card-container {
                grid-template-columns: repeat(3, 1fr);
            }
        }

        /* === [추가] 알고리즘 개별 카드 === */
        .algo-card {
            background-color: var(--color-bg-white);
            border: 1px solid var(--color-border);
            padding: 1.25rem;
            border-radius: 0.5rem;
        }
        .algo-card p {
            margin: 0;
            font-size: 0.9rem;
        }
        
        .model-compare-grid {
            display: grid;
            grid-template-columns: 1fr;
            gap: 1.5rem;
        }

        @media (min-width: 768px) {
            .model-compare-grid {
                grid-template-columns: repeat(2, 1fr);
            }
        }
        
        .model-card h3 {
            font-size: 1.25rem;
            border-bottom: 1px solid var(--color-border);
            padding-bottom: 0.75rem;
            margin-bottom: 1rem;
        }
        .model-card ul {
            padding-left: 20px;
        }

        /* === [추가] 장점/단점 카드 색상 === */
        .bg-light-blue {
            background-color: #f0f7ff;
            border-color: #dbeafe;
        }
        .bg-light-red {
            background-color: #fff1f2;
            border-color: #fecdd3;
        }

        aside {
            background-color: var(--color-bg-white);
            border: 1px solid var(--color-border);
            padding: 1rem 1.5rem;
            margin: 1.5rem 0 0 0;
            border-radius: 0.25rem;
            font-size: 0.95rem;
        }
        
        aside p, aside ul {
            margin-bottom: 0;
            padding-left: 0;
            list-style: none;
        }
        
        .conclusion {
            text-align: center;
            max-width: 700px;
            margin: 3rem auto 0 auto;
            font-size: 1.1rem;
            color: #495057;
        }
    </style>
</head>
<body>

    <main class="container">
        <header class="header">
            <h1>토큰화(Tokenization) 심층 분석</h1>
        </header>

        <section class="card">
            <p><b>토큰화(Tokenization)</b>는 텍스트를 AI 모델이 처리할 수 있는 의미 있는 작은 단위인 <b>토큰(token)</b>으로 나누는 과정입니다.<br>단순히 띄어쓰기로 나누는 것을 넘어, 현대 NLP에서는 OOV 문제를 해결하는 <b>서브워드(Subword)</b> 방식이 표준으로 사용됩니다.</p>
            <aside>
                <p><b>자주 쓰는 단어 (love)</b><br>→ 하나의 토큰 (love)</p>
                <p style="margin-top: 0.5rem;"><b>복잡한 단어 (lovingly)</b><br>→ 여러 토큰의 조합 (love + ##ing + ##ly)</p>
            </aside>
        </section>

        <div class="section-title">
            <h2>단어 토큰화의 한계와 서브워드의 등장</h2>
        </div>
        
        <section class="card">
             <p>기존의 단어 단위(띄어쓰기 기준) 처리 방식은<br><strong>OOV(Out-of-Vocabulary, 어휘집에 없는 단어)</strong> 문제에 취약했습니다.</p>
            <aside>
                <p><strong>문제점</strong><br>
                모델이 <strong>'먹다'</strong>는 알지만 '먹었다'는 모를 경우, '먹었다'를 의미 없는 기호(<code>&lt;UNK&gt;</code>)로 처리해 버립니다.<br> 핵심 의미('먹다')를 알면서도 정보를 놓치는 것이죠. 이는 특히 조사, 어미 활용이 다양한 한국어에서 더 큰 문제였습니다.</p>
                <p style="margin-top: 1rem;"><strong>해결책</strong><br>
                서브워드는 <strong>'먹었다'</strong>를 아는 조각인 <strong>먹 + ##었 + ##다</strong>로 분리합니다.<br> 덕분에 모델은 처음 보는 단어라도 의미를 유추할 수 있게 됩니다.</p>
            </aside>
        </section>

        <div class="section-title">
            <h2>대표적인 알고리즘</h2>
        </div>

        <section class="card">
            <p>이 똑똑한 '단어 쪼개기'는 주로 아래 세 가지 알고리즘을 통해 이루어집니다.<br>이 중 가장 대표적이고 기본이 되는 BPE 알고리즘의 작동 방식을 자세히 살펴보겠습니다.</p>
            <div class="algo-card-container">
                <div class="algo-card">
                    <h4>BPE<br> (Byte Pair Encoding)</h4>
                    <p>글자 단위에서 시작해 <b>가장 자주 함께 나오는</b> 글자 쌍을 합쳐 <b>하나의 토큰</b>으로 만드는 과정을 반복</p>
                </div>
                <div class="algo-card">
                    <h4>WordPiece</h4>
                    <p>BPE와 유사하지만, 빈도수가 아닌 '의미를 가장 잘 보존할 가능성(Likelihood)'이 높은 쌍을 병합 (BERT에서 사용)</p>
                </div>
                <div class="algo-card">
                    <h4>SentencePiece</h4>
                    <p>띄어쓰기까지 하나의 문자로 취급하여 언어에 구애받지 않고 토큰화를 수행</p>
                </div>
            </div>
        </section>
        <div class="section-title">
            <h2>알고리즘 상세 분석: BPE (Byte-Pair Encoding)</h2>
            <p>BPE는 어떻게 최적의 서브워드 어휘집(Vocabulary)을 만들까요?<br><code>low lowest newest</code> 라는 텍스트를 예시로 학습 과정을 살펴보겠습니다.</p>
        </div>
        
        <div class="steps-grid">
            <article class="card step-card">
                <div class="step-number">1</div>
                <div class="step-card-content">
                    <h3>1단계: 준비 (Initialization)</h3>
                    <p>먼저 모든 단어를 글자(character) 단위로 분해하고,<br>단어의 끝을 의미하는 특수 기호 <code>&lt;/w&gt;</code>를 추가합니다.<br>이 시점의 어휘집은 텍스트에 존재하는 모든 기본 글자들입니다.</p>
                    <aside>
                        <p><strong>분절된 텍스트</strong><br>l o w &lt;/w&gt;, l o w e s t &lt;/w&gt;, n e w e s t &lt;/w&gt;</p>
                        <p><strong>초기 어휘집</strong><br>{l, o, w, e, s, t, n, &lt;/w&gt;}</p>
                    </aside>
                </div>
            </article>

            <article class="card step-card">
                <div class="step-number">2</div>
                <div class="step-card-content">
                    <h3>2단계: 반복 병합 (Iterative Merging)</h3>
                    <p>텍스트 전체에서 가장 자주 나타나는 인접한 토큰 쌍을 찾아<br>새로운 토큰으로 병합하고 어휘집에 추가합니다.<br>이 과정을 정해진 횟수만큼 반복합니다.</p>
                    <aside>
                        <p><strong>(1회차)</strong> 'e'와 's' 쌍이 2번으로 가장 빈번 → <code>es</code> 병합<br>
                        텍스트 상태: l o w &lt;/w&gt;, l o w <strong>es</strong> t &lt;/w&gt;, n e w <strong>es</strong> t &lt;/w&gt;</p>
                        <p><strong>(2회차)</strong> 'es'와 't' 쌍이 2번으로 가장 빈번 → <code>est</code> 병합<br>
                        텍스트 상태: l o w &lt;/w&gt;, l o w <strong>est</strong> &lt;/w&gt;, n e w <strong>est</strong> &lt;/w&gt;</p>
                         <p><strong>(3회차)</strong> 'l'과 'o' 쌍이 2번으로 가장 빈번 → <code>lo</code> 병합<br>
                        텍스트 상태: <strong>lo</strong> w &lt;/w&gt;, <strong>lo</strong> w est &lt;/w&gt;, n e w est &lt;/w&gt;</p>
                         <p><strong>(4회차)</strong> 'lo'와 'w' 쌍이 2번으로 가장 빈번 → <code>low</code> 병합<br>
                        텍스트 상태: <strong>low</strong> &lt;/w&gt;, <strong>low</strong> est &lt;/w&gt;, n e w est &lt;/w&gt;</p>
                    </aside>
                </div>
            </article>
        </div>
        
        <div class="section-title">
            <h2>장점과 단점</h2>
        </div>

        <section class="model-compare-grid">
            <div class="model-card card bg-light-blue">
                <h3>장점</h3>
                <ul>
                    <li><strong>OOV 문제 해결</strong><br>모르는 단어를 크게 줄여 모델의 이해력을 높입니다.</li>
                    <li><strong>효율적인 어휘 관리</strong><br>적은 수의 서브워드로 무한한 단어를 표현할 수 있습니다.</li>
                    <li><strong>신조어 및 오타 대응</strong><br>'핵인싸' 같은 단어도 '핵'+'##인싸' 등으로 분리해 의미를 추론합니다.</li>
                    <li><strong>다양한 언어 적용</strong><br>특히 조사와 어미가 발달한 한국어('챗봇'+'##이란')에 효과적입니다.</li>
                </ul>
            </div>
            <div class="model-card card bg-light-red">
                <h3>단점</h3>
                <ul>
                    <li><strong>고유 의미 손실</strong><br>'해바라기'가 '해'+'##바라기'로 나뉘면, 'sunflower'라는 고유한 의미가 약해질 수 있습니다.</li>
                    <li><strong>비직관적 분리</strong><br>'algorithm'이 'al'+'##go'+'##rithm'처럼 통계에만 의존해 의미 없는 조각으로 나뉠 수 있습니다.</li>
                    <li><strong>시퀀스 길이 증가</strong><br>한 단어가 여러 토큰이 되므로 처리 속도가 느려지고, 모델의 최대 입력 길이에 제약이 생길 수 있습니다.</li>
                </ul>
            </div>
        </section>
        <p class="conclusion">
            이러한 단점들에도 불구하고, OOV 문제를 해결하는 이점이 워낙 막강하기 때문에<br><b>현대 NLP 모델들은 서브워드 방식을 표준으로 채택하고 있습니다.</b>
        </p>
    </main>

</body>
</html>
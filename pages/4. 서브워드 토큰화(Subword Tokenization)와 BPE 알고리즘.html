<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>토큰화(Tokenization) 심층 분석</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@400;700;900&display=swap" rel="stylesheet">
    <style>
        /* 1. Core Design Philosophy & 2. Technical Specifications */
        :root {
            --color-text-primary: #212529;
            --color-text-secondary: #6c757d;
            --color-border: #dee2e6;
            --color-bg-white: #ffffff;
            --color-bg-subtle: #f8f9fa;
            --color-step-icon-bg: #2c3e50;
            --font-family-base: 'Noto Sans KR', sans-serif;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: var(--font-family-base);
            font-size: 1rem;
            line-height: 1.7;
            background-color: var(--color-bg-white);
            color: var(--color-text-primary);
            margin: 0;
            padding: 4rem 1rem;
            -webkit-font-smoothing: antialiased;
        }

        /* 2. Typography & Hierarchy */
        h1, h2, h3 {
            font-weight: 700;
            line-height: 1.3;
            margin-top: 0;
        }

        h1 {
            font-size: 2.5rem;
            font-weight: 900;
            margin-bottom: 0.5rem;
        }

        h2 {
            font-size: 2rem;
            margin-bottom: 0.75rem;
        }

        h3 {
            font-size: 1.5rem;
            margin-bottom: 1rem;
        }

        p {
            margin-top: 0;
            margin-bottom: 1rem;
        }
        
        strong, b {
           font-weight: 700;
           color: var(--color-text-primary);
        }
        
        code {
            font-family: Consolas, 'Courier New', monospace;
            background-color: #e9ecef;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-size: 90%;
        }

        /* 3. Component Library & Structure */
        .container {
            max-width: 800px;
            margin: 0 auto;
        }

        .header, .section-title {
            text-align: center;
            margin: 4rem auto 3rem auto;
        }
        
        .header {
            margin-top: 0;
        }
        
        .section-title p {
            font-size: 1.125rem;
            color: var(--color-text-secondary);
            max-width: 600px;
            margin-left: auto;
            margin-right: auto;
            margin-bottom: 0;
        }

        .card {
            background-color: var(--color-bg-white);
            border: 1px solid var(--color-border);
            border-radius: 0.75rem;
            padding: 2.5rem;
            margin-bottom: 1.5rem;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.02);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 20px rgba(0, 0, 0, 0.07);
        }

        .step-card {
            display: flex;
            flex-direction: column;
            gap: 1.5rem;
        }
        
        @media (min-width: 768px) {
            .step-card {
                flex-direction: row;
                align-items: flex-start;
            }
        }
        
        .step-card-content {
            flex: 1;
        }
        
        .step-number {
            flex-shrink: 0;
            width: 50px;
            height: 50px;
            background-color: var(--color-step-icon-bg);
            color: #fff;
            border-radius: 8px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.75rem;
            font-weight: 700;
            text-shadow: 1px 1px 0px rgba(255, 0, 85, 0.5), -1px -1px 0px rgba(0, 229, 255, 0.5);
        }
        
        .model-compare-grid {
            display: grid;
            grid-template-columns: 1fr;
            gap: 1.5rem;
        }

        @media (min-width: 768px) {
            .model-compare-grid {
                grid-template-columns: repeat(2, 1fr);
            }
        }
        
        .model-card h3 {
            font-size: 1.25rem;
            border-bottom: 1px solid var(--color-border);
            padding-bottom: 0.75rem;
            margin-bottom: 1rem;
        }

        aside {
            background-color: var(--color-bg-subtle);
            border-left: 4px solid var(--color-border);
            padding: 1rem 1.5rem;
            margin: 1.5rem 0;
            border-radius: 0 0.25rem 0.25rem 0;
            font-size: 0.95rem;
        }
        
        aside p:last-child, aside ul:last-child {
            margin-bottom: 0;
        }
        
        .conclusion {
            text-align: center;
            max-width: 700px;
            margin: 3rem auto 0 auto;
            font-size: 1.1rem;
            color: #495057;
        }

        .fade-in-up {
            opacity: 0;
            transform: translateY(30px);
            transition: opacity 0.6s ease-out, transform 0.6s ease-out;
        }

        .fade-in-up.visible {
            opacity: 1;
            transform: translateY(0);
        }
    </style>
</head>
<body>

    <main class="container">
        <header class="header fade-in-up">
            <h1>토큰화(Tokenization) 심층 분석</h1>
        </header>

        <section class="content-block card fade-in-up">
            <p><b>토큰화(Tokenization)</b>는 텍스트를 AI 모델이 처리할 수 있는 의미 있는 작은 단위인 <b>토큰(token)</b>으로 나누는 과정입니다. 단순히 띄어쓰기로 나누는 것을 넘어, 현대 NLP에서는 OOV 문제를 해결하는 <b>서브워드(Subword)</b> 방식이 표준으로 사용됩니다.</p>
            <aside>
                <p>자주 쓰는 단어 (love) → <strong>하나의 토큰 (love)</strong><br>
                복잡한 단어 (lovingly) → <strong>여러 토큰의 조합 (love + ##ing + ##ly)</strong></p>
            </aside>
        </section>

        <div class="section-title fade-in-up">
            <h2>단어 토큰화의 한계와 서브워드의 등장</h2>
        </div>
        
        <section class="content-block card fade-in-up">
             <p>기존의 단어 단위(띄어쓰기 기준) 처리 방식은<br> <strong>OOV(Out-of-Vocabulary, 어휘집에 없는 단어)</strong> 문제에 취약했습니다.</p>
            <aside>
                <p><strong>문제점 :</strong><br>
                모델이 <strong>'먹다'</strong>는 알지만 '먹었다'는 모를 경우, '먹었다'를 의미 없는 기호(<code>&lt;UNK&gt;</code>)로 처리해 버립니다.<br> 핵심 의미('먹다')를 알면서도 정보를 놓치는 것이죠. 이는 특히 조사, 어미 활용이 다양한 한국어에서 더 큰 문제였습니다.</p>
                <p><strong>해결책 :</strong><br>
                서브워드는 <strong>'먹었다'</strong>를 아는 조각인 <strong>먹 + ##었 + ##다</strong>로 분리합니다.<br> 덕분에 모델은 처음 보는 단어라도 의미를 유추할 수 있게 됩니다.</p>
            </aside>
        </section>

        <div class="section-title fade-in-up">
            <h2>대표적인 알고리즘</h2>
        </div>

        <section class="content-block card fade-in-up">
            <p>이 똑똑한 '단어 쪼개기'는 주로 아래 세 가지 알고리즘을 통해 이루어집니다.<br> 이 중 가장 대표적이고 기본이 되는 BPE 알고리즘의 작동 방식을 자세히 살펴보겠습니다.</p>
            <aside>
                <ul>
                    <li><strong>BPE (Byte Pair Encoding)</strong><br>: 글자 단위에서 시작해 가장 자주 함께 나오는 글자 쌍을 합쳐 하나의 토큰으로 만드는 과정을 반복</li>
                    <li><strong>WordPiece</strong><br>: BPE와 유사하지만, 단순히 빈도수가 아닌<br> '데이터의 의미를 가장 잘 보존할 가능성(Likelihood)'이 높은 쌍을 병합합니다. (BERT에서 사용)</li>
                    <li><strong>SentencePiece</strong><br>: 띄어쓰기까지 하나의 문자로 취급하여 언어에 구애받지 않고 토큰화를 수행합니다.</li>
                </ul>
            </aside>
        </section>
        
        <div class="section-title fade-in-up">
            <h2>알고리즘 상세 분석 : BPE (Byte-Pair Encoding)</h2>
            <p>BPE는 어떻게 최적의 서브워드 어휘집(Vocabulary)을 만들까요?<br><code>low lowest newest</code> 라는 텍스트를 예시로 학습 과정을 살펴보겠습니다.</p>
        </div>
        
        <div class="steps-grid">
            <article class="card step-card fade-in-up">
                <div class="step-number">1</div>
                <div class="step-card-content">
                    <h3>1단계 : 준비 (Initialization)</h3>
                    <p>먼저 모든 단어를 글자(character) 단위로 분해하고, 단어의 끝을 의미하는 특수 기호 <code>&lt;/w&gt;</code>를<br> 추가합니다. 이 시점의 어휘집은 텍스트에 존재하는 모든 기본 글자들입니다.</p>
                    <aside>
                        <p><strong>분절된 텍스트 :</strong> l o w &lt;/w&gt;, l o w e s t &lt;/w&gt;, n e w e s t &lt;/w&gt;<br>
                        <strong>초기 어휘집 :</strong> {l, o, w, e, s, t, n, &lt;/w&gt;}</p>
                    </aside>
                </div>
            </article>

            <article class="card step-card fade-in-up">
                <div class="step-number">2</div>
                <div class="step-card-content">
                    <h3>2단계 : 반복 병합 (Iterative Merging)</h3>
                    <p>텍스트 전체에서 가장 자주 나타나는 인접한 토큰 쌍을 찾아<br> 새로운 토큰으로 병합하고 어휘집에 추가합니다. 이 과정을 정해진 횟수만큼 반복합니다.</p>
                    <aside>
                        <p><strong>(1회차)</strong> e와 s 쌍이 2번으로 가장 빈번합니다. → <code>es</code>를 병합하고 어휘집에 추가합니다.<br>
                        텍스트 상태 : l o w &lt;/w&gt;, l o w <strong>es</strong> t &lt;/w&gt;, n e w <strong>es</strong> t &lt;/w&gt;</p>
                        <p><strong>(2회차)</strong> es와 t 쌍이 2번으로 가장 빈번합니다. → <code>est</code>를 병합하고 어휘집에 추가합니다.<br>
                        텍스트 상태 : l o w &lt;/w&gt;, l o w <strong>est</strong> &lt;/w&gt;, n e w <strong>est</strong> &lt;/w&gt;</p>
                         <p><strong>(3회차)</strong> l과 o 쌍이 2번으로 가장 빈번합니다. → <code>lo</code>를 병합하고 어휘집에 추가합니다.<br>
                        텍스트 상태 : <strong>lo</strong> w &lt;/w&gt;, <strong>lo</strong> w est &lt;/w&gt;, n e w est &lt;/w&gt;</p>
                         <p><strong>(4회차)</strong> lo와 w 쌍이 2번으로 가장 빈번합니다. → <code>low</code>를 병합하고 어휘집에 추가합니다.<br>
                        텍스트 상태 : <strong>low</strong> &lt;/w&gt;, <strong>low</strong> est &lt;/w&gt;, n e w est &lt;/w&gt;</p>
                    </aside>
                </div>
            </article>
        </div>
        
        <div class="section-title fade-in-up">
            <h2>장점과 단점</h2>
        </div>

        <section class="model-compare-grid">
            <div class="model-card card fade-in-up">
                <h3>👍 장점</h3>
                <ul>
                    <li><strong>OOV 문제 해결</strong><br> : 모르는 단어를 크게 줄여 모델의 이해력을 높입니다.</li>
                    <li><strong>효율적인 어휘 관리</strong><br> : 적은 수의 서브워드로 무한한 단어를 표현할 수 있습니다.</li>
                    <li><strong>신조어 및 오타 대응</strong><br> : '핵인싸' 같은 단어도 핵+##인싸 등으로 분리해 의미를 추론합니다.</li>
                    <li><strong>다양한 언어 적용</strong><br> : 특히 조사와 어미가 발달한 한국어(챗봇+##이란)에 효과적입니다.</li>
                </ul>
            </div>
            <div class="model-card card highlight-gray fade-in-up">
                <h3>👎 단점</h3>
                <ul>
                    <li><strong>고유 의미 손실</strong><br>: '해바라기'가 해+##바라기로 나뉘면, 'sunflower'라는 고유한 의미가 약해질 수 있습니다.</li>
                    <li><strong>비직관적 분리</strong><br>: algorithm이 al+##go+##rithm처럼 통계에만 의존해 의미 없는 조각으로 나뉠 수 있습니다.</li>
                    <li><strong>시퀀스 길이 증가</strong><br>: 한 단어가 여러 토큰이 되므로 처리 속도가 느려지고, 모델의 최대 입력 길이에 제약이 생길 수 있습니다.</li>
                </ul>
            </div>
        </section>

        <p class="conclusion fade-in-up">
            이러한 단점들에도 불구하고, OOV 문제를 해결하는 이점이 워낙 막강하기 때문에<br> <b>현대 NLP 모델들은 서브워드 방식을 표준으로 채택하고 있습니다.</b>
        </p>
    </main>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const animatedElements = document.querySelectorAll('.fade-in-up');

            if ('IntersectionObserver' in window) {
                const observer = new IntersectionObserver((entries, observer) => {
                    entries.forEach(entry => {
                        if (entry.isIntersecting) {
                            entry.target.classList.add('visible');
                            observer.unobserve(entry.target);
                        }
                    });
                }, {
                    threshold: 0.1
                });

                animatedElements.forEach(el => {
                    observer.observe(el);
                });
            } else {
                animatedElements.forEach(el => {
                    el.classList.add('visible');
                });
            }
        });
    </script>

</body>
</html>
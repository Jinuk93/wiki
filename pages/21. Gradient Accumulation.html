<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gradient Accumulation: VRAM 한계 극복하기</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@400;500;700&display=swap" rel="stylesheet">
    
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <style>
        body {
            font-family: 'Noto Sans KR', sans-serif;
            background-color: #f7f8fc;
            color: #495057;
            margin: 0;
            padding: 0;
            line-height: 1.7;
            -webkit-font-smoothing: antialiased;
        }

        .container {
            max-width: 900px;
            margin: 40px auto;
            padding: 0 20px;
        }

        h1 {
            font-size: 2.5rem;
            font-weight: 700;
            color: #212529;
            text-align: center;
            margin-bottom: 12px;
        }

        p.subtitle {
            font-size: 1.15rem;
            color: #6c757d;
            text-align: center;
            margin-top: 0;
            margin-bottom: 40px;
        }

        h2 {
            font-size: 1.75rem;
            font-weight: 700;
            color: #212529;
            text-align: center;
            margin: 60px 0 30px 0;
        }

        h3 {
            font-size: 1.4rem;
            font-weight: 700;
            color: #212529;
            margin-top: 0;
            margin-bottom: 15px; /* Increased bottom margin */
        }

        h4 {
            font-size: 1.1rem;
            font-weight: 700;
            color: #343a40;
            margin-top: 0;
            margin-bottom: 12px;
        }

        p {
            margin-top: 0;
            margin-bottom: 1rem;
            color: #495057;
        }
        
        p.secondary {
             color: #6c757d;
             font-size: 1.05rem;
        }

        /* Use <b> tag directly in HTML for emphasis */
        b { 
            font-weight: 700;
            color: #343a40;
        }

        hr {
            border: 0;
            height: 1px;
            background-color: #dee2e6;
            margin: 60px 0;
        }

        .card {
            border: 1px solid #e9ecef;
            border-radius: 12px;
            margin-bottom: 24px;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.04);
            overflow: hidden;
        }

        .conceptual-card {
            background-color: #f8f9fa;
            padding: 24px 30px;
            transition: transform 0.2s ease, box-shadow 0.2s ease;
        }

        .conceptual-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 8px 20px rgba(0, 0, 0, 0.08);
        }

        .sub-card {
            background-color: #ffffff;
            border: 1px solid #e9ecef;
            border-radius: 8px;
            padding: 20px 24px;
            margin-top: 20px;
            box-shadow: 0 2px 6px rgba(0, 0, 0, 0.03);
        }
        
        .side-by-side-container {
            display: flex;
            gap: 20px;
            margin-top: 20px;
        }

        .side-by-side-container > .sub-card {
            flex: 1;
            margin-top: 0;
        }
        
        pre {
            background-color: #f5f7fa;
            border: 1px solid #e0e6ed;
            border-radius: 6px;
            padding: 16px;
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
            font-size: 0.9rem;
            line-height: 1.6;
            overflow-x: auto;
            margin-top: 10px;
            margin-bottom: 1rem;
        }
        
        @media (max-width: 768px) {
            .side-by-side-container {
                flex-direction: column;
            }
            h1 { font-size: 2rem; }
            h2 { font-size: 1.5rem; }
        }
    </style>
</head>
<body>

    <div class="container">

        <h1>Gradient Accumulation</h1>
        <p class="subtitle">제한된 GPU 메모리로 큰 배치 사이즈 효과를 내는 방법</p>

        <hr>
        <h2>섹션 1: 모델 학습과 '배치 사이즈(Batch Size)'의 딜레마</h2>

        <div class="card conceptual-card">
            <h3>배치 사이즈란 무엇이고 왜 중요할까</h3>
            <p class="secondary">모델을 학습시킬 때는 보통 전체 데이터를 한 번에 처리하지 않고, 작은 묶음 단위로 나누어 처리합니다. 이 묶음의 크기를 <b>'배치 사이즈(Batch Size)'</b>라고 합니다.</p>
            
            <div class="sub-card">
                <h4>배치 사이즈 정의</h4>
                <p>모델이 한 번 가중치를 업데이트(학습)하기 위해 사용하는 데이터 샘플의 개수입니다.</p>
                <p>예를 들어 배치 사이즈가 256이라면, 256개의 데이터를 보고 나서야 "아, 이 방향으로 가야겠구나" 하고 모델의 파라미터를 한 번 수정합니다. (Forward & Backward Pass)</p>
            </div>
            
            <div class="side-by-side-container">
                 <div class="sub-card">
                      <h4>크면 좋다 (일반적으로)</h4>
                      <p>배치 사이즈가 크면, 더 많은 데이터를 종합해서 학습 방향(기울기, Gradient)을 결정하기 때문에, 방향이 비교적 <b>안정적</b>이고 학습이 부드럽게 진행될 가능성이 높습니다.</p>
                 </div>
                 <div class="sub-card">
                      <h4>작으면 불안정하다</h4>
                      <p>반대로 배치 사이즈가 너무 작으면, 소수의 데이터만 보고 학습 방향을 결정해야 해서 그 방향이 <b>매우 불안정하고 들쭉날쭉(noisy)</b>할 수 있습니다. 이는 학습을 불안정하게 만들고 최적점에 도달하기 어렵게 만들 수 있습니다.</p>
                 </div>
            </div>

             <div class="sub-card">
                 <h4>현실적인 문제 GPU 메모리(VRAM)의 한계</h4>
                 <p>하지만 트랜스포머처럼 모델의 크기가 매우 커지면서, 한 번에 처리해야 하는 데이터(배치)를 GPU 메모리에 올리는 것 자체가 부담이 됩니다.</p>
                 <p>결국, GPU의 <b>VRAM 용량 한계</b> 때문에 우리가 원하는 만큼 <b>큰 배치 사이즈를 사용하기 어려운</b> 현실적인 제약에 부딪히게 됩니다. 특히 고성능 GPU가 부족할 경우 이 문제는 더 심각해집니다.</p>
             </div>
        </div>

        <hr>
        <h2>섹션 2: VRAM 한계를 극복하는 마법 - 'Gradient Accumulation'</h2>

        <div class="card conceptual-card">
            <h3>작은 배치로 큰 배치 효과 내기</h3>
             <p class="secondary">여기서 <b>'Gradient Accumulation(기울기 누적)'</b>이라는 아주 영리한 기법이 등장합니다. VRAM이 부족해서 큰 배치 사이즈를 쓸 수 없을 때, <b>마치 큰 배치 사이즈를 쓰는 것과 같은 효과</b>를 내는 방법입니다.</p>
             
             <div class="sub-card">
                 <h4>핵심 아이디어</h4>
                 <p>"100kg짜리 역기를 한 번에 못 들면, 10kg짜리를 10번 나눠 들고 그 힘을 합치면 되지 않을까?"</p>
                 <p>마찬가지로, VRAM에 올라가는 <b>작은 배치(예: 32)</b>에 대한 학습 방향(기울기)을 <b>여러 번 계산</b>합니다. (예: 8번)</p>
                 <p>중요한 것은, 이때 <b>모델 파라미터를 바로 업데이트하지 않고</b>, 계산된 기울기들을 <b>차곡차곡 더해서(누적해서)</b> 모아둡니다.</p>
                 <p>정해진 횟수(예: 8번)만큼 기울기를 모았다면, 그때 <b>모아둔 기울기를 이용해 모델 파라미터를 딱 한 번 업데이트</b>합니다.</p>
             </div>

             <div class="sub-card">
                 <h4>결과</h4>
                 <p>GPU는 실제로는 작은 배치(32)만 처리했지만, 모델 파라미터 업데이트는 <b>8번 누적된 기울기</b>를 사용했기 때문에, 마치 <b>큰 배치(32 * 8 = 256)를 사용한 것과 동일한 효과</b>를 얻게 됩니다.</p>
                 <p>이를 통해 VRAM 한계를 극복하고, 작은 배치 사이즈의 단점인 <b>불안정한 기울기(noisy gradient) 문제를 완화</b>하여 학습 안정성을 높일 수 있습니다.</p>
             </div>
        </div>

        <hr>
        <h2>섹션 3: Gradient Accumulation 작동 방식 (훈련 루프의 변화)</h2>

        <div class="card conceptual-card">
            <h3>기울기 누적 방식의 학습 과정</h3>
             <p class="secondary">Gradient Accumulation을 사용하면 기존의 모델 훈련 방식(루프)이 약간 달라집니다.</p>
             
             <div class="side-by-side-container">
                 <div class="sub-card">
                      <h4>기존 방식 (매 스텝 업데이트)</h4>
                      <p>1. 배치 가져오기<br>
                      2. 예측 (Feed-forward)<br>
                      3. 손실 계산<br>
                      4. 기울기 계산 (Back-propagation)<br>
                      5. <b>파라미터 업데이트</b><br>
                      6. 기울기 초기화<br>
                      7. (반복)</p>
                 </div>
                 <div class="sub-card">
                      <h4>Gradient Accumulation (k 스텝마다 업데이트)</h4>
                      <p>k = Accumulation Steps (예: 8)</p>
                      <p>1. <b>k번 반복 시작</b><br>
                      &nbsp;&nbsp;&nbsp;&nbsp;a. 미니배치 가져오기<br>
                      &nbsp;&nbsp;&nbsp;&nbsp;b. 예측 (Feed-forward)<br>
                      &nbsp;&nbsp;&nbsp;&nbsp;c. 손실 계산 (loss / k)<br>
                      &nbsp;&nbsp;&nbsp;&nbsp;d. 기울기 계산 및 <b>누적</b><br>
                      2. <b>k번 반복 끝</b><br>
                      3. <b>파라미터 업데이트 (누적된 기울기 사용!)</b><br>
                      4. 기울기 초기화<br>
                      5. (반복)</p>
                 </div>
             </div>
             
             <div class="sub-card">
                  <h4>주의할 점</h4>
                  <p>Gradient Accumulation은 <b>학습 속도를 빠르게 해주지는 않습니다.</b> 오히려 작은 배치를 여러 번 처리해야 하므로 약간 더 느릴 수 있습니다. 이 기법의 목적은 <b>속도 향상이 아니라, 제한된 하드웨어에서 큰 배치 사이즈의 효과를 얻어 학습 안정성을 높이는 것</b>입니다.</p>
             </div>
        </div>

        <hr>
        <h2>섹션 4: 활용 및 계산</h2>

        <div class="card conceptual-card">
             <h3>실제 적용 및 효과 계산</h3>
             <p class="secondary">Gradient Accumulation은 특히 거대 언어 모델(LLM)처럼 VRAM 요구량이 큰 모델을 학습시킬 때 매우 유용하게 사용됩니다.</p>
             
             <div class="sub-card">
                  <h4>더 큰 배치 사이즈 효과 계산</h4>
                  <p>만약 여러 개의 GPU를 사용한다면 (Data Parallelism), <b>실제 총 배치 사이즈</b>는 다음과 같이 계산됩니다.</p>
                  <pre>Total Batch Size = #GPUs × Gradient Accumulation Steps × Per Device Train Batch Size</pre>
                  <p>(GPU 개수 * 누적 스텝 수 * GPU당 배치 사이즈)</p>
                  <p>이를 통해 단일 GPU로는 상상할 수 없었던 매우 큰 배치 사이즈 효과를 낼 수 있습니다.</p>
             </div>
             
             <div class="sub-card">
                  <h4>파라미터 업데이트 횟수 계산</h4>
                  <p>총 학습 데이터 수(#Samples), 학습 에폭 수(#Epochs), 그리고 위에서 계산한 총 배치 사이즈(Total Batch Size)를 알면, 전체 학습 과정 동안 <b>모델 파라미터가 총 몇 번 업데이트되는지</b> 계산할 수 있습니다.</p>
                  <pre>#Param Updates = (#Epochs × #Samples) ÷ Total Batch Size</pre>
                  <p>이는 학습 시간 예측이나 다른 연구와의 비교 등에 유용하게 사용됩니다.</p>
             </div>
             
             <div class="sub-card">
                  <h4>간편한 사용 (Hugging Face)</h4>
                   <p>다행히 Hugging Face의 <b>transformers</b> 라이브러리 같은 곳에서는 <b>TrainingArguments</b>에 <b>gradient_accumulation_steps</b> 인자만 설정해주면 이 복잡한 과정을 자동으로 처리해줍니다.</p>
                   <pre># 예시
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=32,
    gradient_accumulation_steps=8, # 8스텝 동안 기울기 누적
    # ... 다른 인자들
)
</pre>
             </div>
        </div>

        <hr>
        <h2>섹션 5: 요약</h2>

        <div class="card conceptual-card">
            <h3>Gradient Accumulation의 핵심 역할</h3>
            <p class="secondary">Gradient Accumulation은 다음과 같은 핵심적인 역할을 합니다.</p>
            
             <div class="sub-card">
                 <p><b>1. GPU VRAM 한계 극복</b><br>
                 물리적인 메모리 제약을 넘어 <b>논리적으로 큰 배치 사이즈</b>를 사용할 수 있게 해줍니다.</p>
            </div>
            <div class="sub-card">
                 <p><b>2. 학습 안정성 향상</b><br>
                 작은 배치 사이즈로 인한 <b>불안정한 기울기(noisy gradient) 문제를 완화</b>하여 모델 학습을 더 안정적으로 만듭니다.</p>
            </div>
            <div class="sub-card">
                 <p><b>3. 속도 향상은 아님</b><br>
                 처리 시간은 약간 더 늘어날 수 있지만, 안정적인 학습을 통해 <b>결과적으로는 더 좋은 모델</b>을 얻을 가능성을 높입니다.</p>
            </div>
            
            <p class="secondary">따라서 Gradient Accumulation은 특히 트랜스포머와 같은 거대 모델을 학습시키는 데 있어 거의 필수적인 테크닉 중 하나로 자리 잡았습니다.</p>
        </div>

    </div>

</body>
</html>